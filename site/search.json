[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MODELEX.CO",
    "section": "",
    "text": "Extreme Learning Machine\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nExtreme Learning Machine\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLasso\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nLasso\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nNelder Mead\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nNelder Mead\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nLinear Regression\n\n\nOLS\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nRidge\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nRidge\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nCubic Splines\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nCubic Splines\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "R code to estimate a linear model\n\nset.seed(123)  # ensures replication\n\n# predictors and response\nN = 100 # sample size\nk = 2   # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)  \ny = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5)  # increasing N will get estimated values closer to these\n\ndfXy = data.frame(X, y)\n\n\n\n#'\n#' # Functions \n#'\n\n#' A maximum likelihood approach.\nlm_ML = function(par, X, y) {\n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta   = par[-1]                             # coefficients\n  sigma2 = par[1]                              # error variance\n  sigma  = sqrt(sigma2)\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link in the glm sense\n  \n  # calculate likelihood\n  L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood\n#   L =  -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu)    # alternate log likelihood form\n\n  -sum(L)                                      # optim by default is minimization, and we want to maximize the likelihood \n                                               # (see also fnscale in optim.control)\n}\n\n# An approach via least squares loss function.\n\nlm_LS = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link\n  \n  # calculate least squares loss function\n  L = crossprod(y - mu)\n}\n\n\n#' #  Obtain Model Estimates\n#'\n#' Setup for use with optim.\n\nX = cbind(1, X)\n\n#' Initial values. Note we'd normally want to handle the sigma differently as\n#' it's bounded by zero, but we'll ignore for demonstration.  Also sigma2 is not\n#' required for the LS approach as it is the objective function.\n\ninit = c(1, rep(0, ncol(X)))\nnames(init) = c('sigma2', 'intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = lm_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptlmLS = optim(\n  par = init[-1],\n  fn  = lm_LS,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\npars_ML = optlmML$par\npars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par)  # calculate sigma2 and add\n\n\n\n#' #  Comparison\n#' \n#' Compare to `lm` which uses QR decomposition.\n\nmodlm = lm(y ~ ., dfXy)\n\n#' Example\n#' \n# QRX = qr(X)\n# Q = qr.Q(QRX)\n# R = qr.R(QRX)\n# Bhat = solve(R) %*% crossprod(Q, y)\n# alternate: qr.coef(QRX, y)\n\nround(\n  rbind(\n    pars_ML,\n    pars_LS,\n    modlm = c(summary(modlm)$sigma^2, coef(modlm))), \n  digits = 3\n)\n\n        sigma2 intercept    b1    b2\npars_ML  0.219    -0.432 0.133 0.112\npars_LS  0.226    -0.432 0.133 0.112\nmodlm    0.226    -0.432 0.133 0.112\n\n#' The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in\n#' the traditional least squares approach; diminishes with increasing N as both\n#' tend toward whatever sd^2 you specify when creating the y response above.\n\n\n#'\n#'Compare to glm, which by default assumes gaussian family with identity link\n#'and uses `lm.fit`.\n#'\nmodglm = glm(y ~ ., data = dfXy)\nsummary(modglm)\n\n\nCall:\nglm(formula = y ~ ., data = dfXy)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.93651  -0.33037  -0.06222   0.31068   1.03991  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.43247    0.04807  -8.997 1.97e-14 ***\nX1           0.13341    0.05243   2.544   0.0125 *  \nX2           0.11191    0.04950   2.261   0.0260 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.2262419)\n\n    Null deviance: 24.444  on 99  degrees of freedom\nResidual deviance: 21.945  on 97  degrees of freedom\nAIC: 140.13\n\nNumber of Fisher Scoring iterations: 2\n\n#' Via normal equations.\ncoefs = solve(t(X) %*% X) %*% t(X) %*% y  # coefficients\n\n#' Compare.\n#' \nsqrt(crossprod(y - X %*% coefs) / (N - k - 1))\n\n          [,1]\n[1,] 0.4756489\n\nsummary(modlm)$sigma\n\n[1] 0.4756489\n\nsqrt(modglm$deviance / modglm$df.residual) \n\n[1] 0.4756489\n\nc(sqrt(pars_ML[1]), sqrt(pars_LS[1]))\n\n   sigma2    sigma2 \n0.4684616 0.4756490 \n\n# rerun by adding 3-4 zeros to the N\n\nThis code uses gradient descent to find the beta0 and beta1 coefficients that minimize the mean squared error (MSE). The learning rate alpha can be adjusted to influence the speed of learning."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "R code to estimate a logistic regression model\n\nset.seed(1235)  # ensures replication\n\n\n# predictors and target\n\nN = 10000 # sample size\nk = 2     # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)\n\n# the linear predictor\nlp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these\n\ny = rbinom(N, size = 1, prob = plogis(lp))\n\ndfXy = data.frame(X, y)\n\n\n\n#' \n#' # Functions \n#' \n#' A maximum likelihood approach.\n\nlogreg_ML = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                # coefficients\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                           # linear predictor\n  mu = plogis(LP)                           # logit link\n  \n  # calculate likelihood\n  L = dbinom(y, size = 1, prob = mu, log = TRUE)         # log likelihood\n  #   L =  y*log(mu) + (1 - y)*log(1-mu)    # alternate log likelihood form\n  \n  -sum(L)                                   # optim by default is minimization, and we want to maximize the likelihood \n  # (see also fnscale in optim.control)\n}\n\n# An equivalent approach via exponential loss function.\n\nlogreg_exp = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n\n  # calculate exponential loss function (convert y to -1:1 from 0:1)\n  L = sum(exp(-ifelse(y, 1, -1) * .5 * LP))\n}\n\n\n#' # Obtain Model Estimates\n#' Setup for use with `optim`.\n\nX = cbind(1, X)\n\n# initial values\n\ninit = rep(0, ncol(X))\nnames(init) = c('intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = logreg_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptglmClass = optim(\n  par = init,\n  fn  = logreg_exp,\n  X   = X,\n  y   = y, \n  control = list(reltol = 1e-15)\n)\n\npars_ML  = optlmML$par\npars_exp = optglmClass$par\n\n\n#' # Comparison\n#' \n#' Compare to `glm`.\n\nmodglm = glm(y ~ ., dfXy, family = binomial)\n\nrbind(\n  pars_ML,\n  pars_exp,\n  pars_GLM = coef(modglm)\n)\n\n          intercept        b1         b2\npars_ML  -0.5117658 0.2378927 0.08019841\npars_exp -0.5114284 0.2368478 0.07907056\npars_GLM -0.5117321 0.2378743 0.08032617"
  },
  {
    "objectID": "posts/MIXED/index.html",
    "href": "posts/MIXED/index.html",
    "title": "Lasso",
    "section": "",
    "text": "lasso <- function(\n  X,                   # model matrix\n  y,                   # target\n  lambda  = .1,        # penalty parameter\n  soft    = TRUE,      # soft vs. hard thresholding\n  tol     = 1e-6,      # tolerance\n  iter    = 100,       # number of max iterations\n  verbose = TRUE       # print out iteration number\n) {\n  \n  # soft thresholding function\n  soft_thresh <- function(a, b) {\n    out = rep(0, length(a))\n    out[a >  b] = a[a > b] - b\n    out[a < -b] = a[a < -b] + b\n    out\n  }\n  \n  w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y)\n  tol_curr = 1\n  J = ncol(X)\n  a = rep(0, J)\n  c_ = rep(0, J)\n  i = 1\n  \n  while (tol < tol_curr && i < iter) {\n    w_old = w \n    a = colSums(X^2)\n    l = length(y)*lambda  # for consistency with glmnet approach\n    c_ = sapply(1:J, function(j)  sum( X[,j] * (y - X[,-j] %*% w_old[-j]) ))\n    if (soft) {\n      for (j in 1:J) {\n        w[j] = soft_thresh(c_[j]/a[j], l/a[j])\n      }\n    }\n    else {\n      w = w_old\n      w[c_< l & c_ > -l] = 0\n    }\n    \n    tol_curr = crossprod(w - w_old)  \n    i = i + 1\n    if (verbose && i%%10 == 0) message(i)\n  }\n  \n  w\n}\n\n#' # Data setup\n#' \n#' \nset.seed(8675309)\nN = 500\np = 10\nX = scale(matrix(rnorm(N*p), ncol=p))\nb = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6))\ny = scale(X %*% b + rnorm(N, sd=.5))\nlambda = .1\n\n\n# debugonce(lasso)\n\n#' Note, if `lambda=0`, result is the same as  `lm.fit`.\n#' \n#' \nresult_soft = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol = 1e-12,\n  soft = TRUE\n)\n\nresult_hard = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol    = 1e-12,\n  soft   = FALSE\n)\n\n\n\n\n#' `glmnet` is by default a mixture of ridge and lasso penalties, setting alpha\n#' = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple\n#' values while only wanting the one set to the same lambda value as above (s).\n\n\nlibrary(glmnet)\n\nLe chargement a nécessité le package : Matrix\n\n\nLoaded glmnet 4.1-6\n\nglmnet_res = coef(\n  glmnet(\n    X,\n    y,\n    alpha  = 1,\n    lambda = c(10, 1, lambda),\n    thresh = 1e-12,\n    intercept = FALSE\n  ),\n  s = lambda\n)\n\nlibrary(lassoshooting)\n\nls_res = lassoshooting(\n  X = X,\n  y = y,\n  lambda = length(y) * lambda,\n  thr = 1e-12\n)\n\n\n#' # Comparison\n\ndata.frame(\n  lm = coef(lm(y ~ . - 1, data.frame(X))),\n  lasso_soft = result_soft,\n  lasso_hard = result_hard,\n  lspack = ls_res$coef,\n  glmnet = glmnet_res[-1, 1],\n  truth  = b\n)\n\n              lm  lasso_soft lasso_hard      lspack      glmnet  truth\nX1   0.534988063  0.43542527  0.5348784  0.43542528  0.43552489  0.500\nX2  -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500\nX3   0.234376590  0.12436834  0.2343207  0.12436835  0.12447920  0.250\nX4  -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250\nX5   0.126037566  0.02036410  0.1260132  0.02036407  0.02047015  0.125\nX6  -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125\nX7  -0.016718534  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX8   0.009894575  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX9  -0.005441959  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX10  0.010561128  0.00000000  0.0000000  0.00000000  0.00000000  0.000"
  },
  {
    "objectID": "posts/ridge/index.html",
    "href": "posts/ridge/index.html",
    "title": "Ridge",
    "section": "",
    "text": "ridge <- function(w, X, y, lambda = .1) {\n  # X: model matrix; \n  # y: target; \n  # lambda: penalty parameter; \n  # w: the weights/coefficients\n  \n  crossprod(y - X %*% w) + lambda * length(y) * crossprod(w)\n}\n\n\nset.seed(8675309)\nN = 500\np = 10\nX = scale(matrix(rnorm(N * p), ncol = p))\nb = c(.5, -.5, .25, -.25, .125, -.125, rep(0, 4))\ny = scale(X %*% b + rnorm(N, sd = .5))\n\n#' Note, if `lambda=0`, result is the same as  `lm.fit`.\n#' \n#' \nresult_ridge = optim(\n  rep(0, ncol(X)),\n  ridge,\n  X = X,\n  y = y,\n  lambda = .1,\n  method = 'BFGS'\n)\n\n#' Analytical result.\n#' \nresult_ridge2 =  solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y)\n\n#' Alternative with augmented data (note sigma ignored as it equals 1, but otherwise\n#' X/sigma and y/sigma).\n#' \nX2 = rbind(X, diag(sqrt(length(y)*.1), ncol(X)))\ny2 = c(y, rep(0, ncol(X)))\nresult_ridge3 = solve(crossprod(X2)) %*% crossprod(X2, y2)\n\n\n\n\n#' `glmnet` is by default a mixture of ridge and lasso penalties, setting alpha\n#' = 1 reduces to lasso, while alpha=0 would be ridge.\n\n\nlibrary(glmnet)\n\nLe chargement a nécessité le package : Matrix\n\n\nLoaded glmnet 4.1-6\n\nglmnet_res = coef(\n  glmnet(\n    X,\n    y,\n    alpha = 0,\n    lambda = c(10, 1, .1),\n    thresh = 1e-12,\n    intercept = F\n  ), \n  s = .1\n)\n\n#' # Comparison\n\ndata.frame(\n  lm     = coef(lm(y ~ . - 1, data.frame(X))),\n  ridge  = result_ridge$par,\n  ridge2 = result_ridge2,\n  ridge3 = result_ridge3,\n  glmnet = glmnet_res[-1, 1],\n  truth  = b\n)\n\n              lm        ridge       ridge2       ridge3       glmnet  truth\nX1   0.534988063  0.485323748  0.485323748  0.485323748  0.485368766  0.500\nX2  -0.529993422 -0.480742032 -0.480742032 -0.480742032 -0.480786661 -0.500\nX3   0.234376590  0.209412833  0.209412833  0.209412833  0.209435147  0.250\nX4  -0.294350608 -0.268814168 -0.268814168 -0.268814168 -0.268837476 -0.250\nX5   0.126037566  0.114963716  0.114963716  0.114963716  0.114973801  0.125\nX6  -0.159386728 -0.145880488 -0.145880488 -0.145880488 -0.145892837 -0.125\nX7  -0.016718534 -0.021658889 -0.021658889 -0.021658889 -0.021655033  0.000\nX8   0.009894575  0.006956965  0.006956965  0.006956965  0.006959470  0.000\nX9  -0.005441959  0.001392244  0.001392244  0.001392244  0.001386661  0.000\nX10  0.010561128  0.010985385  0.010985385  0.010985385  0.010985102  0.000"
  },
  {
    "objectID": "posts/neldermead/index.html",
    "href": "posts/neldermead/index.html",
    "title": "Nelder Mead",
    "section": "",
    "text": "nelder_mead = function(\n  f, \n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha    = 1,\n  gamma    = 2,\n  rho      = 0.5,\n  sigma    = 0.5,\n  verbose  = FALSE\n  ) {\n  # init\n  dim = length(x_start)\n  prev_best = f(x_start)\n  no_improv = 0\n  res = list(list(x_start = x_start, prev_best = prev_best))\n  \n  \n  for (i in 1:dim) {\n    x = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res = append(res, list(list(x_start = x, prev_best = score)))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    idx  = sapply(res, `[[`, 2)\n    res  = res[order(idx)]   # ascending order\n    best = res[[1]][[2]]\n    \n    # break after max_iter\n    if (max_iter > 0 & iters >= max_iter) return(res[[1]])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break) return(res[[1]])\n    \n    # centroid\n    x0 = rep(0, dim)\n    for (tup in 1:(length(res)-1)) {\n      for (i in 1:dim) {\n        x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1)\n      }\n    }\n    \n   # reflection\n   xr = x0 + alpha * (x0 - res[[length(res)]][[1]])\n   rscore = f(xr)\n   if (res[[1]][[2]] <= rscore & \n       rscore < res[[length(res)-1]][[2]]) {\n     res[[length(res)]] = list(xr, rscore)\n     next\n   }\n     \n   # expansion\n   if (rscore < res[[1]][[2]]) {\n     # xe = x0 + gamma*(x0 - res[[length(res)]][[1]])   # issue with this\n     xe = x0 + gamma * (xr - x0)   \n     escore = f(xe)\n     if (escore < rscore) {\n       res[[length(res)]] = list(xe, escore)\n       next\n     } else {\n       res[[length(res)]] = list(xr, rscore)\n       next\n     }\n   }\n   \n   # contraction\n   # xc = x0 + rho*(x0 - res[[length(res)]][[1]])  # issue with wiki consistency for rho values (and optim)\n   xc = x0 + rho * (res[[length(res)]][[1]] - x0)\n   cscore = f(xc)\n   if (cscore < res[[length(res)]][[2]]) {\n     res[[length(res)]] = list(xc, cscore)\n     next\n   }\n   \n   # reduction\n   x1   = res[[1]][[1]]\n   nres = list()\n   for (tup in res) {\n     redx  = x1 + sigma * (tup[[1]] - x1)\n     score = f(redx)\n     nres  = append(nres, list(list(redx, score)))\n   }\n   \n   res = nres\n  }\n}\n\n\n\n\n#' ## Example\n#' The function to minimize.\n#' \nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\n[[1]]\n[1] -1.570797e+00 -2.235577e-07  1.637460e-14\n\n[[2]]\n[1] -1\n\n#' Compare to `optimx`.  You may see warnings.\noptimx::optimx(\n  par = c(0, 0, 0),\n  fn = f,\n  method = \"Nelder-Mead\",\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta = 0.5,\n    maxit = 1000,\n    reltol = 1e-12\n  )\n)\n\nWarning in max(logpar): aucun argument pour max ; -Inf est renvoyé\n\n\nWarning in min(logpar): aucun argument trouvé pour min ; Inf est renvoyé\n\n\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n\n#' ## A Regression Model\n#' \n#' I find a regression model to be more applicable/intuitive for my needs, so\n#' provide an example for that case.\n#' \n#' \n#' ### Data setup\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n\n#' Least squares loss function.\n\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n# lm estimates\nlm.fit(X, y)$coef\n\n         x1          x2          x3          x4          x5          x6 \n-0.96214657  0.59432481  0.04864576  0.27573466  0.97525840 -0.07470287 \n\nnm_result = nelder_mead(\n  f, \n  runif(ncol(X)), \n  max_iter = 2000,\n  no_improve_thr = 1e-12,\n  verbose = FALSE\n)\n\n#' ### Comparison\n#' Compare to `optimx`.\n\nopt_out = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,  # model function\n  method  = 'Nelder-Mead',\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta  = 0.5,\n    #rho\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)\n\nrbind(\n  nm_func = unlist(nm_result),\n  nm_optimx = opt_out[1:7]\n)\n\n                  p1       p2         p3        p4        p5          p6\nnm_func   -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389\nnm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054\n             value\nnm_func   501.3155\nnm_optimx 501.3155\n\n#' # Second version\n#' \n#' This is a more natural R approach in my opinion.\n\nnelder_mead2 = function(\n  f,\n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha = 1,\n  gamma = 2,\n  rho   = 0.5,\n  sigma = 0.5,\n  verbose = FALSE\n) {\n  \n  # init\n  npar = length(x_start)\n  nc = npar + 1\n  prev_best = f(x_start)\n  no_improv = 0\n  res = matrix(c(x_start, prev_best), ncol = nc)\n  colnames(res) = c(paste('par', 1:npar, sep = '_'), 'score')\n  \n  for (i in 1:npar) {\n    x     = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res   = rbind(res, c(x, score))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    res  = res[order(res[, nc]), ]   # ascending order\n    best = res[1, nc]\n    \n    # break after max_iter\n    if (max_iter & iters >= max_iter) return(res[1, ])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break)\n      return(res[1, ])\n    \n    nr = nrow(res)\n    \n    # centroid: more efficient than previous double loop\n    x0 = colMeans(res[(1:npar), -nc])\n    \n    # reflection\n    xr = x0 + alpha * (x0 - res[nr, -nc])\n    \n    rscore = f(xr)\n    \n    if (res[1, 'score'] <= rscore & rscore < res[npar, 'score']) {\n      res[nr,] = c(xr, rscore)\n      next\n    }\n    \n    # expansion\n    if (rscore < res[1, 'score']) {\n      xe = x0 + gamma * (xr - x0)\n      escore = f(xe)\n      if (escore < rscore) {\n        res[nr, ] = c(xe, escore)\n        next\n      } else {\n        res[nr, ] = c(xr, rscore)\n        next\n      }\n    }\n    \n    # contraction\n    xc = x0 + rho * (res[nr, -nc] - x0)\n    \n    cscore = f(xc)\n    \n    if (cscore < res[nr, 'score']) {\n      res[nr,] = c(xc, cscore)\n      next\n    }\n    \n    # reduction\n    x1 = res[1, -nc]\n    \n    nres = res\n    \n    for (i in 1:nr) {\n      redx  = x1 + sigma * (res[i, -nc] - x1)\n      score = f(redx)\n      nres[i, ] = c(redx, score)\n    }\n    \n    res = nres\n  }\n}\n\n\n#' ## Example function\n\nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead2(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\n        par_1         par_2         par_3         score \n-1.570797e+00 -2.235577e-07  1.622809e-14 -1.000000e+00 \n\noptimx::optimx(\n  par = c(0, 0, 0), \n  fn = f, \n  method   = \"Nelder-Mead\",\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 1000,\n    reltol = 1e-12\n  )\n)\n\nWarning in max(logpar): aucun argument pour max ; -Inf est renvoyé\n\nWarning in max(logpar): aucun argument trouvé pour min ; Inf est renvoyé\n\n\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n\n#' ## A Regression Model\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n# least squares loss\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n\nlm_par = lm.fit(X, y)$coef\n\nnm_par = nelder_mead2(\n  f, \n  runif(ncol(X)),\n  max_iter = 2000,\n  no_improve_thr = 1e-12\n)\n\n\n#' ## Comparison\n#' Compare to `optimx`.\n\nopt_par = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,\n  method   = 'Nelder-Mead',\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)[1:(npreds + 1)]\n\nrbind(\n  lm = lm_par,\n  nm = nm_par,\n  optimx = opt_par,\n  truth  = beta\n)\n\nWarning in rbind(deparse.level, ...): number of columns of result, 6, is not a\nmultiple of vector length 7 of arg 2\n\n\n               p1        p2         p3        p4        p5          p6\nlm     -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287\nnm     -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389\noptimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054\ntruth  -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885"
  },
  {
    "objectID": "posts/spline/index.html",
    "href": "posts/spline/index.html",
    "title": "Cubic Splines",
    "section": "",
    "text": "library(tidyverse) # for processing and plotting\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n#' # Create the data\nsize = c(1.42,1.58,1.78,1.99,1.99,1.99,2.13,2.13,2.13,\n         2.32,2.32,2.32,2.32,2.32,2.43,2.43,2.78,2.98,2.98)\n\nwear = c(4.0,4.2,2.5,2.6,2.8,2.4,3.2,2.4,2.6,4.8,2.9,\n         3.8,3.0,2.7,3.1,3.3,3.0,2.8,1.7)\n\nx = size - min(size)\nx = x / max(x)\nd = data.frame(wear, x)\n\n#' Cubic spline function\nrk <- function(x, z) {\n  ((z-0.5)^2 - 1/12) * ((x-0.5)^2 - 1/12)/4 -\n    ((abs(x-z)-0.5)^4 - (abs(x-z)-0.5)^2/2 + 7/240) / 24\n}\n\n#' Generate the model matrix.\nsplX <- function(x, knots) {\n  q = length(knots) + 2                # number of parameters\n  n = length(x)                        # number of observations\n  X = matrix(1, n, q)                  # initialized model matrix\n  X[ ,2]   = x                         # set second column to x\n  X[ ,3:q] = outer(x, knots, FUN = rk) # remaining to cubic spline basis\n  X\n}\n\nsplS <- function(knots) {\n  q = length(knots) + 2\n  S = matrix(0, q, q)                         # initialize matrix\n  S[3:q, 3:q] = outer(knots, knots, FUN = rk) # fill in non-zero part\n  S\n}\n\n#' Matrix square root function. Note that there are various packages with their own.\nmatSqrt <- function(S) {\n  d  = eigen(S, symmetric = T)\n  rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors)\n  rS\n}\n\n#' Penalized fitting function.\nprsFit <- function(y, x, knots, lambda) {\n  q  = length(knots) + 2    # dimension of basis\n  n  = length(x)            # number of observations\n  Xa = rbind(splX(x, knots), matSqrt(splS(knots))*sqrt(lambda)) # augmented model matrix\n  y[(n+1):(n+q)] = 0        # augment the data vector\n  \n  lm(y ~ Xa - 1) # fit and return penalized regression spline\n}\n\n\n\n#' # Example 1\n\n\n#' Unpenalized\n#' \nknots = 1:4/5\nX = splX(x, knots)      # generate model matrix\nmod1 = lm(wear ~ X - 1) # fit model\n\nxp = 0:100/100 # x values for prediction\nXp = splX(xp, knots) # prediction matrix\n\n\n#' Visualize\n\nggplot(aes(x = x, y = wear), data = data.frame(x, wear)) +\n  geom_point(color = \"#FF5500\") +\n  geom_line(aes(x = xp, y = Xp %*% coef(mod1)),\n            data = data.frame(xp, Xp),\n            color = \"#00AAFF\") +\n  labs(x = 'Scaled Engine size', y  = 'Wear Index') +\n  theme_minimal()\n\n\n\n#' # Example 2\n\n\n# Add penalty lambda\nknots = 1:7/8\nd2 = data.frame(x = xp)\n\nfor (i in c(.1, .01, .001, .0001, .00001, .000001)){\n  # fit penalized regression\n  mod2 = prsFit(\n    y = wear,\n    x = x,\n    knots = knots,\n    lambda = i\n  ) \n  # spline choosing lambda\n  Xp = splX(xp, knots) # matrix to map parameters to fitted values at xp\n  LP = Xp %*% coef(mod2)\n  d2[, paste0('lambda = ', i)] = LP[, 1]\n}\n\n#' Examine\n# head(d2)\n\n#' Visualize via ggplot\nd3 = d2 %>%\n  pivot_longer(cols = -x,\n               names_to  = 'lambda',\n               values_to = 'value') %>% \n  mutate(lambda = fct_inorder(lambda))\n\nggplot(d3) +\n  geom_point(aes(x = x, y = wear), col = '#FF5500', data = d) +\n  geom_line(aes(x = x, y = value), col = \"#00AAFF\") +\n  facet_wrap(~lambda) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/ELM/index.html",
    "href": "posts/ELM/index.html",
    "title": "Extreme Learning Machine",
    "section": "",
    "text": "elm <- function(X, y, n_hidden=NULL, active_fun=tanh) {\n  # X: an N observations x p features matrix\n  # y: the target\n  # n_hidden: the number of hidden nodes\n  # active_fun: activation function\n  pp1 = ncol(X) + 1\n  w0 = matrix(rnorm(pp1*n_hidden), pp1, n_hidden)       # random weights\n  h = active_fun(cbind(1, scale(X)) %*% w0)             # compute hidden layer\n  B = MASS::ginv(h) %*% y                               # find weights for hidden layer\n  fit = h %*% B                                         # fitted values\n  list(fit= fit, loss=crossprod(fit - y), B=B, w0=w0)\n}\n\n\n\n# one variable, complex function -------------------------------------------\nlibrary(tidyverse); library(mgcv)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLe chargement a nécessité le package : nlme\n\n\nAttachement du package : 'nlme'\n\n\nL'objet suivant est masqué depuis 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n\nset.seed(123)\nn = 5000\nx = runif(n)\n# x = rnorm(n)\nmu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))\ny = rnorm(n, mu, .3)\n# qplot(x, y)\nd = data.frame(x,y) \n\nX_ = as.matrix(x, ncol=1)\n\ntest = elm(X_, y, n_hidden=100)\nstr(test)\n\nList of 4\n $ fit : num [1:5000, 1] -1.0239 0.7311 -0.413 0.0806 -0.4112 ...\n $ loss: num [1, 1] 442\n $ B   : num [1:100, 1] 217 -608 1408 -1433 -4575 ...\n $ w0  : num [1:2, 1:100] 0.35 0.814 -0.517 -2.692 -1.097 ...\n\n# qplot(x, y) + geom_line(aes(y=test$fit), color='#1e90ff')\ncor(test$fit[,1], y)^2\n\n[1] 0.8862518\n\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n\n[1] 0.8482127\n\nd %>% \n  mutate(fit_elm = test$fit,\n         fit_gam = fitted(gam_comparison)) %>% \n  ggplot() + \n  geom_point(aes(x, y), alpha=.1) +\n  geom_line(aes(x, y=fit_elm), color='#1e90ff') + \n  geom_line(aes(x, y=fit_gam), color='darkred')\n\n\n\n# motorcycle accident data ------------------------------------------------\n\ndata('mcycle', package='MASS')\nx = mcycle[,1]\nX_ = matrix(x, ncol=1)\ny = mcycle[,2]\n\ntest = elm(X_, y, n_hidden=100)\ncor(test$fit[,1], y)^2\n\n[1] 0.8122349\n\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n\n[1] 0.7832988\n\nqplot(x, y) +\n  geom_line(aes(y=test$fit), color='#1e90ff') + \n  geom_line(aes(y=fitted(gam_comparison)), color='darkred')\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n# add covariates ----------------------------------------------------------\n\nd = gamSim(eg=7, n=10000)\n\nGu & Wahba 4 term additive model, correlated predictors\n\nX_ = as.matrix(d[,2:5])\ny = d[,1]\n\nn_nodes = c(10, 25, 100, 250, 500, 1000)\ntest = lapply(n_nodes, function(n) elm(X_, y, n_hidden=n))       # this will take a few seconds\nfinal_n = which.min(sapply(test, function(x) x$loss))\nbest = test[[final_n]]\n# str(best)\nqplot(best$fit[,1], y, alpha=.2)\n\n\n\ncor(best$fit[,1], y)^2\n\n[1] 0.7241967\n\ngam_comparison = gam(y~s(x0) + s(x1) + s(x2) + s(x3), data=d)\ngam.check(gam_comparison)\n\n\n\n\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 15 iterations.\nThe RMS GCV score gradient at convergence was 9.309879e-07 .\nThe Hessian was positive definite.\nModel rank =  37 / 37 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n        k'  edf k-index p-value\ns(x0) 9.00 4.71    1.00    0.48\ns(x1) 9.00 4.89    1.00    0.35\ns(x2) 9.00 8.96    0.99    0.20\ns(x3) 9.00 1.00    1.00    0.47\n\nsummary(gam_comparison)$r.sq\n\n[1] 0.6952978\n\ntest_data0 = gamSim(eg=7)  # default n = 400\n\nGu & Wahba 4 term additive model, correlated predictors\n\ntest_data =  cbind(1, scale(test_data0[,2:5]))\n\nelm_prediction = tanh(test_data %*% best$w0) %*% best$B          # remember to use your specific activation function here\ngam_prediction = predict(gam_comparison, newdata=test_data0)\ncor(data.frame(elm_prediction, gam_prediction), test_data0$y)^2\n\n                    [,1]\nelm_prediction 0.6873090\ngam_prediction 0.7185687"
  }
]