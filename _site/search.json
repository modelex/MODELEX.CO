[
  {
    "objectID": "posts/SGD/index.html",
    "href": "posts/SGD/index.html",
    "title": "Stochastic Gradient Descent",
    "section": "",
    "text": "set.seed(1234)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)\n\n\n\n#' # Stochastic Gradient Descent Algorithm\n\n\nsgd = function(\n  par,                                      # parameter estimates\n  X,                                        # model matrix\n  y,                                        # target variable\n  stepsize = 1,                             # the learning rate\n  stepsizeTau = 0,                          # if > 0, a check on the LR at early iterations\n  average = FALSE\n){\n  \n  # initialize\n  beta = par\n  names(beta) = colnames(X)\n  betamat = matrix(0, nrow(X), ncol = length(beta))      # Collect all estimates\n  fits = NA                                              # fitted values\n  s = 0                                                  # adagrad per parameter learning rate adjustment\n  loss = NA                                              # Collect loss at each point\n  \n  for (i in 1:nrow(X)) {\n    Xi   = X[i, , drop = FALSE]\n    yi   = y[i]\n    LP   = Xi %*% beta                                   # matrix operations not necessary, \n    grad = t(Xi) %*% (LP - yi)                           # but makes consistent with the  standard gd R file\n    s    = s + grad^2\n    beta = beta - stepsize * grad/(stepsizeTau + sqrt(s))     # adagrad approach\n    \n    if (average & i > 1) {\n      beta =  beta - 1/i * (betamat[i - 1, ] - beta)          # a variation\n    } \n    \n    betamat[i,] = beta\n    fits[i]     = LP\n    loss[i]     = (LP - yi)^2\n  }\n  \n  LP = X %*% beta\n  lastloss = crossprod(LP - y)\n  \n  list(\n    par    = beta,                                       # final estimates\n    parvec = betamat,                                    # all estimates\n    loss   = loss,                                       # observation level loss\n    RMSE   = sqrt(sum(lastloss)/nrow(X)),\n    fitted = fits\n  )\n}\n\n\n\n#' # Run\n\n\n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you might have to fiddle with the `stepsize`, perhaps\n#' choosing one based on cross-validation with old data.\n\nsgd_result = sgd(\n  init,\n  X = X,\n  y = y,\n  stepsize = .1,\n  stepsizeTau = .5,\n  average = FALSE\n)\n\nstr(sgd_result)\n\nList of 5\n $ par   : num [1:3, 1] 1.024 0.537 0.148\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ parvec: num [1:1000, 1:3] -0.06208 -0.00264 0.04781 0.09866 0.08242 ...\n $ loss  : num [1:1000] 0.67 1.261 1.365 2.043 0.215 ...\n $ RMSE  : num 1.01\n $ fitted: num [1:1000] 0 -0.0236 -0.0446 -0.2828 0.1634 ...\n\nsgd_result$par\n\n               [,1]\nIntercept 1.0241049\nx1        0.5368198\nx2        0.1478470\n\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n#' \n# summary(lm(y ~ x1 + x2))\ncoef1 = coef(lm(y ~ x1 + x2))\n\nrbind(\n  sgd_result = sgd_result$par[, 1],\n  lm = coef1\n)\n\n           Intercept        x1        x2\nsgd_result  1.024105 0.5368198 0.1478470\nlm          1.029957 0.5177020 0.1631026\n\n#' ## Visualize Estimates\n#' \nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\ngd = data.frame(sgd_result$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>%\n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter',\n               values_to = 'Value') %>%\n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\nggplot(aes(\n  x = Iteration,\n  y = Value,\n  group = Parameter,\n  color = Parameter\n),\ndata = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration == n), size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    size  = 4,\n    data  = filter(gd, Iteration == n)\n  ) +\n  theme_minimal()\n\n\n\n#' # Add alternately data shift\n\n#' This data includes a shift of the previous data.\n\nset.seed(1234)\n\nn2   = 1000\nx1.2 = rnorm(n2)\nx2.2 = rnorm(n2)\ny2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2)\nX2 = rbind(X, cbind(1, x1.2, x2.2))\ncoef2 = coef(lm(y2 ~ x1.2 + x2.2))\ny2 = c(y, y2)\n\nn3    = 1000\nx1.3  = rnorm(n3)\nx2.3  = rnorm(n3)\ny3    = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3)\ncoef3 = coef(lm(y3 ~ x1.3 + x2.3))\n\nX3 = rbind(X2, cbind(1, x1.3, x2.3))\ny3 = c(y2, y3)\n\n\n\n#' ## Run\n\n\nsgd_result2 = sgd(\n  init,\n  X = X3,\n  y = y3,\n  stepsize = 1,\n  stepsizeTau = 0,\n  average = FALSE\n)\n\nstr(sgd_result2)\n\nList of 5\n $ par   : num [1:3, 1] 0.821 -0.223 0.211\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ parvec: num [1:3000, 1:3] -1 -0.119 0.624 1.531 1.063 ...\n $ loss  : num [1:3000] 0.67 2.31 3.69 30.99 10.58 ...\n $ RMSE  : num 1.57\n $ fitted: num [1:3000] 0 -0.421 -0.797 -4.421 2.952 ...\n\n#' Compare with `lm` for each data part.\n#' \nsgd_result2$parvec[c(n, n + n2, n + n2 + n3), ]\n\n           [,1]       [,2]       [,3]\n[1,]  1.0859378  0.5128904  0.1457697\n[2,] -0.9246994  0.2945723 -0.2941759\n[3,]  0.8213521 -0.2229918  0.2112883\n\nrbind(coef1, coef2, coef3)\n\n      (Intercept)         x1         x2\ncoef1   1.0299573  0.5177020  0.1631026\ncoef2  -0.9700427  0.2677020 -0.2868974\ncoef3   1.0453166 -0.2358521  0.2418489\n\n#' Visualize estimates.\n#' \ngd = data.frame(sgd_result2$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>% \n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter', \n               values_to = 'Value') %>% \n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\n\nggplot(aes(x = Iteration,\n           y = Value,\n           group = Parameter,\n           color = Parameter\n           ),\n       data = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n             size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n    size = 4,\n    show.legend = FALSE\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MODELEX.CO",
    "section": "",
    "text": "Basic Neural Network\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nBasic Neural Network\n\n\nNeural Network\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nExtreme Learning Machine\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nExtreme Learning Machine\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nGradient Descent\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nGradient Descent\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nGaussian Process\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nGaussian Process\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLasso\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nLasso\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nLinear Regression\n\n\nOLS\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nMaximum Likelihood\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nMaximum Likelihood\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nNelder Mead\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nNelder Mead\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nRidge\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nRidge\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nStochastic Gradient Descent\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nGradient Descent\n\n\nStochastic Gradient Descent\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\n  \n\n\n\n\nCubic Splines\n\n\n\n\n\n\n\nR\n\n\nCode\n\n\nCubic Splines\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nSimon-Pierre Boucher\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/ELM/index.html",
    "href": "posts/ELM/index.html",
    "title": "Extreme Learning Machine",
    "section": "",
    "text": "elm <- function(X, y, n_hidden=NULL, active_fun=tanh) {\n  # X: an N observations x p features matrix\n  # y: the target\n  # n_hidden: the number of hidden nodes\n  # active_fun: activation function\n  pp1 = ncol(X) + 1\n  w0 = matrix(rnorm(pp1*n_hidden), pp1, n_hidden)       # random weights\n  h = active_fun(cbind(1, scale(X)) %*% w0)             # compute hidden layer\n  B = MASS::ginv(h) %*% y                               # find weights for hidden layer\n  fit = h %*% B                                         # fitted values\n  list(fit= fit, loss=crossprod(fit - y), B=B, w0=w0)\n}\n\n\n\n# one variable, complex function -------------------------------------------\nlibrary(tidyverse); library(mgcv)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLe chargement a nécessité le package : nlme\n\n\nAttachement du package : 'nlme'\n\n\nL'objet suivant est masqué depuis 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n\nset.seed(123)\nn = 5000\nx = runif(n)\n# x = rnorm(n)\nmu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))\ny = rnorm(n, mu, .3)\n# qplot(x, y)\nd = data.frame(x,y) \n\nX_ = as.matrix(x, ncol=1)\n\ntest = elm(X_, y, n_hidden=100)\nstr(test)\n\nList of 4\n $ fit : num [1:5000, 1] -1.0239 0.7311 -0.413 0.0806 -0.4112 ...\n $ loss: num [1, 1] 442\n $ B   : num [1:100, 1] 217 -608 1408 -1433 -4575 ...\n $ w0  : num [1:2, 1:100] 0.35 0.814 -0.517 -2.692 -1.097 ...\n\n# qplot(x, y) + geom_line(aes(y=test$fit), color='#1e90ff')\ncor(test$fit[,1], y)^2\n\n[1] 0.8862518\n\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n\n[1] 0.8482127\n\nd %>% \n  mutate(fit_elm = test$fit,\n         fit_gam = fitted(gam_comparison)) %>% \n  ggplot() + \n  geom_point(aes(x, y), alpha=.1) +\n  geom_line(aes(x, y=fit_elm), color='#1e90ff') + \n  geom_line(aes(x, y=fit_gam), color='darkred')\n\n\n\n# motorcycle accident data ------------------------------------------------\n\ndata('mcycle', package='MASS')\nx = mcycle[,1]\nX_ = matrix(x, ncol=1)\ny = mcycle[,2]\n\ntest = elm(X_, y, n_hidden=100)\ncor(test$fit[,1], y)^2\n\n[1] 0.8122349\n\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n\n[1] 0.7832988\n\nqplot(x, y) +\n  geom_line(aes(y=test$fit), color='#1e90ff') + \n  geom_line(aes(y=fitted(gam_comparison)), color='darkred')\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n# add covariates ----------------------------------------------------------\n\nd = gamSim(eg=7, n=10000)\n\nGu & Wahba 4 term additive model, correlated predictors\n\nX_ = as.matrix(d[,2:5])\ny = d[,1]\n\nn_nodes = c(10, 25, 100, 250, 500, 1000)\ntest = lapply(n_nodes, function(n) elm(X_, y, n_hidden=n))       # this will take a few seconds\nfinal_n = which.min(sapply(test, function(x) x$loss))\nbest = test[[final_n]]\n# str(best)\nqplot(best$fit[,1], y, alpha=.2)\n\n\n\ncor(best$fit[,1], y)^2\n\n[1] 0.7241967\n\ngam_comparison = gam(y~s(x0) + s(x1) + s(x2) + s(x3), data=d)\ngam.check(gam_comparison)\n\n\n\n\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 15 iterations.\nThe RMS GCV score gradient at convergence was 9.309879e-07 .\nThe Hessian was positive definite.\nModel rank =  37 / 37 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n        k'  edf k-index p-value\ns(x0) 9.00 4.71    1.00    0.48\ns(x1) 9.00 4.89    1.00    0.35\ns(x2) 9.00 8.96    0.99    0.20\ns(x3) 9.00 1.00    1.00    0.47\n\nsummary(gam_comparison)$r.sq\n\n[1] 0.6952978\n\ntest_data0 = gamSim(eg=7)  # default n = 400\n\nGu & Wahba 4 term additive model, correlated predictors\n\ntest_data =  cbind(1, scale(test_data0[,2:5]))\n\nelm_prediction = tanh(test_data %*% best$w0) %*% best$B          # remember to use your specific activation function here\ngam_prediction = predict(gam_comparison, newdata=test_data0)\ncor(data.frame(elm_prediction, gam_prediction), test_data0$y)^2\n\n                    [,1]\nelm_prediction 0.6873090\ngam_prediction 0.7185687"
  },
  {
    "objectID": "posts/MIXED/index.html",
    "href": "posts/MIXED/index.html",
    "title": "Lasso",
    "section": "",
    "text": "lasso <- function(\n  X,                   # model matrix\n  y,                   # target\n  lambda  = .1,        # penalty parameter\n  soft    = TRUE,      # soft vs. hard thresholding\n  tol     = 1e-6,      # tolerance\n  iter    = 100,       # number of max iterations\n  verbose = TRUE       # print out iteration number\n) {\n  \n  # soft thresholding function\n  soft_thresh <- function(a, b) {\n    out = rep(0, length(a))\n    out[a >  b] = a[a > b] - b\n    out[a < -b] = a[a < -b] + b\n    out\n  }\n  \n  w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y)\n  tol_curr = 1\n  J = ncol(X)\n  a = rep(0, J)\n  c_ = rep(0, J)\n  i = 1\n  \n  while (tol < tol_curr && i < iter) {\n    w_old = w \n    a = colSums(X^2)\n    l = length(y)*lambda  # for consistency with glmnet approach\n    c_ = sapply(1:J, function(j)  sum( X[,j] * (y - X[,-j] %*% w_old[-j]) ))\n    if (soft) {\n      for (j in 1:J) {\n        w[j] = soft_thresh(c_[j]/a[j], l/a[j])\n      }\n    }\n    else {\n      w = w_old\n      w[c_< l & c_ > -l] = 0\n    }\n    \n    tol_curr = crossprod(w - w_old)  \n    i = i + 1\n    if (verbose && i%%10 == 0) message(i)\n  }\n  \n  w\n}\n\n#' # Data setup\n#' \n#' \nset.seed(8675309)\nN = 500\np = 10\nX = scale(matrix(rnorm(N*p), ncol=p))\nb = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6))\ny = scale(X %*% b + rnorm(N, sd=.5))\nlambda = .1\n\n\n# debugonce(lasso)\n\n#' Note, if `lambda=0`, result is the same as  `lm.fit`.\n#' \n#' \nresult_soft = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol = 1e-12,\n  soft = TRUE\n)\n\nresult_hard = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol    = 1e-12,\n  soft   = FALSE\n)\n\n\n\n\n#' `glmnet` is by default a mixture of ridge and lasso penalties, setting alpha\n#' = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple\n#' values while only wanting the one set to the same lambda value as above (s).\n\n\nlibrary(glmnet)\n\nLe chargement a nécessité le package : Matrix\n\n\nLoaded glmnet 4.1-6\n\nglmnet_res = coef(\n  glmnet(\n    X,\n    y,\n    alpha  = 1,\n    lambda = c(10, 1, lambda),\n    thresh = 1e-12,\n    intercept = FALSE\n  ),\n  s = lambda\n)\n\nlibrary(lassoshooting)\n\nls_res = lassoshooting(\n  X = X,\n  y = y,\n  lambda = length(y) * lambda,\n  thr = 1e-12\n)\n\n\n#' # Comparison\n\ndata.frame(\n  lm = coef(lm(y ~ . - 1, data.frame(X))),\n  lasso_soft = result_soft,\n  lasso_hard = result_hard,\n  lspack = ls_res$coef,\n  glmnet = glmnet_res[-1, 1],\n  truth  = b\n)\n\n              lm  lasso_soft lasso_hard      lspack      glmnet  truth\nX1   0.534988063  0.43542527  0.5348784  0.43542528  0.43552489  0.500\nX2  -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500\nX3   0.234376590  0.12436834  0.2343207  0.12436835  0.12447920  0.250\nX4  -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250\nX5   0.126037566  0.02036410  0.1260132  0.02036407  0.02047015  0.125\nX6  -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125\nX7  -0.016718534  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX8   0.009894575  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX9  -0.005441959  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX10  0.010561128  0.00000000  0.0000000  0.00000000  0.00000000  0.000"
  },
  {
    "objectID": "posts/neldermead/index.html",
    "href": "posts/neldermead/index.html",
    "title": "Nelder Mead",
    "section": "",
    "text": "nelder_mead = function(\n  f, \n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha    = 1,\n  gamma    = 2,\n  rho      = 0.5,\n  sigma    = 0.5,\n  verbose  = FALSE\n  ) {\n  # init\n  dim = length(x_start)\n  prev_best = f(x_start)\n  no_improv = 0\n  res = list(list(x_start = x_start, prev_best = prev_best))\n  \n  \n  for (i in 1:dim) {\n    x = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res = append(res, list(list(x_start = x, prev_best = score)))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    idx  = sapply(res, `[[`, 2)\n    res  = res[order(idx)]   # ascending order\n    best = res[[1]][[2]]\n    \n    # break after max_iter\n    if (max_iter > 0 & iters >= max_iter) return(res[[1]])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break) return(res[[1]])\n    \n    # centroid\n    x0 = rep(0, dim)\n    for (tup in 1:(length(res)-1)) {\n      for (i in 1:dim) {\n        x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1)\n      }\n    }\n    \n   # reflection\n   xr = x0 + alpha * (x0 - res[[length(res)]][[1]])\n   rscore = f(xr)\n   if (res[[1]][[2]] <= rscore & \n       rscore < res[[length(res)-1]][[2]]) {\n     res[[length(res)]] = list(xr, rscore)\n     next\n   }\n     \n   # expansion\n   if (rscore < res[[1]][[2]]) {\n     # xe = x0 + gamma*(x0 - res[[length(res)]][[1]])   # issue with this\n     xe = x0 + gamma * (xr - x0)   \n     escore = f(xe)\n     if (escore < rscore) {\n       res[[length(res)]] = list(xe, escore)\n       next\n     } else {\n       res[[length(res)]] = list(xr, rscore)\n       next\n     }\n   }\n   \n   # contraction\n   # xc = x0 + rho*(x0 - res[[length(res)]][[1]])  # issue with wiki consistency for rho values (and optim)\n   xc = x0 + rho * (res[[length(res)]][[1]] - x0)\n   cscore = f(xc)\n   if (cscore < res[[length(res)]][[2]]) {\n     res[[length(res)]] = list(xc, cscore)\n     next\n   }\n   \n   # reduction\n   x1   = res[[1]][[1]]\n   nres = list()\n   for (tup in res) {\n     redx  = x1 + sigma * (tup[[1]] - x1)\n     score = f(redx)\n     nres  = append(nres, list(list(redx, score)))\n   }\n   \n   res = nres\n  }\n}\n\n\n\n\n#' ## Example\n#' The function to minimize.\n#' \nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\n[[1]]\n[1] -1.570797e+00 -2.235577e-07  1.637460e-14\n\n[[2]]\n[1] -1\n\n#' Compare to `optimx`.  You may see warnings.\noptimx::optimx(\n  par = c(0, 0, 0),\n  fn = f,\n  method = \"Nelder-Mead\",\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta = 0.5,\n    maxit = 1000,\n    reltol = 1e-12\n  )\n)\n\nWarning in max(logpar): aucun argument pour max ; -Inf est renvoyé\n\n\nWarning in min(logpar): aucun argument trouvé pour min ; Inf est renvoyé\n\n\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n\n#' ## A Regression Model\n#' \n#' I find a regression model to be more applicable/intuitive for my needs, so\n#' provide an example for that case.\n#' \n#' \n#' ### Data setup\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n\n#' Least squares loss function.\n\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n# lm estimates\nlm.fit(X, y)$coef\n\n         x1          x2          x3          x4          x5          x6 \n-0.96214657  0.59432481  0.04864576  0.27573466  0.97525840 -0.07470287 \n\nnm_result = nelder_mead(\n  f, \n  runif(ncol(X)), \n  max_iter = 2000,\n  no_improve_thr = 1e-12,\n  verbose = FALSE\n)\n\n#' ### Comparison\n#' Compare to `optimx`.\n\nopt_out = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,  # model function\n  method  = 'Nelder-Mead',\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta  = 0.5,\n    #rho\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)\n\nrbind(\n  nm_func = unlist(nm_result),\n  nm_optimx = opt_out[1:7]\n)\n\n                  p1       p2         p3        p4        p5          p6\nnm_func   -0.9621510 0.594327 0.04864183 0.2757265 0.9752524 -0.07470389\nnm_optimx -0.9621494 0.594325 0.04864620 0.2757383 0.9752579 -0.07470054\n             value\nnm_func   501.3155\nnm_optimx 501.3155\n\n#' # Second version\n#' \n#' This is a more natural R approach in my opinion.\n\nnelder_mead2 = function(\n  f,\n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha = 1,\n  gamma = 2,\n  rho   = 0.5,\n  sigma = 0.5,\n  verbose = FALSE\n) {\n  \n  # init\n  npar = length(x_start)\n  nc = npar + 1\n  prev_best = f(x_start)\n  no_improv = 0\n  res = matrix(c(x_start, prev_best), ncol = nc)\n  colnames(res) = c(paste('par', 1:npar, sep = '_'), 'score')\n  \n  for (i in 1:npar) {\n    x     = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res   = rbind(res, c(x, score))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    res  = res[order(res[, nc]), ]   # ascending order\n    best = res[1, nc]\n    \n    # break after max_iter\n    if (max_iter & iters >= max_iter) return(res[1, ])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break)\n      return(res[1, ])\n    \n    nr = nrow(res)\n    \n    # centroid: more efficient than previous double loop\n    x0 = colMeans(res[(1:npar), -nc])\n    \n    # reflection\n    xr = x0 + alpha * (x0 - res[nr, -nc])\n    \n    rscore = f(xr)\n    \n    if (res[1, 'score'] <= rscore & rscore < res[npar, 'score']) {\n      res[nr,] = c(xr, rscore)\n      next\n    }\n    \n    # expansion\n    if (rscore < res[1, 'score']) {\n      xe = x0 + gamma * (xr - x0)\n      escore = f(xe)\n      if (escore < rscore) {\n        res[nr, ] = c(xe, escore)\n        next\n      } else {\n        res[nr, ] = c(xr, rscore)\n        next\n      }\n    }\n    \n    # contraction\n    xc = x0 + rho * (res[nr, -nc] - x0)\n    \n    cscore = f(xc)\n    \n    if (cscore < res[nr, 'score']) {\n      res[nr,] = c(xc, cscore)\n      next\n    }\n    \n    # reduction\n    x1 = res[1, -nc]\n    \n    nres = res\n    \n    for (i in 1:nr) {\n      redx  = x1 + sigma * (res[i, -nc] - x1)\n      score = f(redx)\n      nres[i, ] = c(redx, score)\n    }\n    \n    res = nres\n  }\n}\n\n\n#' ## Example function\n\nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead2(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\n        par_1         par_2         par_3         score \n-1.570797e+00 -2.235577e-07  1.622809e-14 -1.000000e+00 \n\noptimx::optimx(\n  par = c(0, 0, 0), \n  fn = f, \n  method   = \"Nelder-Mead\",\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 1000,\n    reltol = 1e-12\n  )\n)\n\nWarning in max(logpar): aucun argument pour max ; -Inf est renvoyé\n\nWarning in max(logpar): aucun argument trouvé pour min ; Inf est renvoyé\n\n\n                   p1           p2           p3 value fevals gevals niter\nNelder-Mead -1.570796 1.394018e-08 1.088215e-16    -1    861     NA    NA\n            convcode kkt1 kkt2 xtime\nNelder-Mead        0 TRUE TRUE     0\n\n#' ## A Regression Model\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n# least squares loss\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n\nlm_par = lm.fit(X, y)$coef\n\nnm_par = nelder_mead2(\n  f, \n  runif(ncol(X)),\n  max_iter = 2000,\n  no_improve_thr = 1e-12\n)\n\n\n#' ## Comparison\n#' Compare to `optimx`.\n\nopt_par = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,\n  method   = 'Nelder-Mead',\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)[1:(npreds + 1)]\n\nrbind(\n  lm = lm_par,\n  nm = nm_par,\n  optimx = opt_par,\n  truth  = beta\n)\n\nWarning in rbind(deparse.level, ...): number of columns of result, 6, is not a\nmultiple of vector length 7 of arg 2\n\n\n               p1        p2         p3        p4        p5          p6\nlm     -0.9621466 0.5943248 0.04864576 0.2757347 0.9752584 -0.07470287\nnm     -0.9621510 0.5943270 0.04864183 0.2757265 0.9752524 -0.07470389\noptimx -0.9621494 0.5943250 0.04864620 0.2757383 0.9752579 -0.07470054\ntruth  -0.9087584 0.6195267 0.07358131 0.3196977 0.9561050 -0.07977885"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "R code to estimate a linear model\n\nset.seed(123)  # ensures replication\n\n# predictors and response\nN = 100 # sample size\nk = 2   # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)  \ny = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5)  # increasing N will get estimated values closer to these\n\ndfXy = data.frame(X, y)\n\n\n\n#'\n#' # Functions \n#'\n\n#' A maximum likelihood approach.\nlm_ML = function(par, X, y) {\n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta   = par[-1]                             # coefficients\n  sigma2 = par[1]                              # error variance\n  sigma  = sqrt(sigma2)\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link in the glm sense\n  \n  # calculate likelihood\n  L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood\n#   L =  -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu)    # alternate log likelihood form\n\n  -sum(L)                                      # optim by default is minimization, and we want to maximize the likelihood \n                                               # (see also fnscale in optim.control)\n}\n\n# An approach via least squares loss function.\n\nlm_LS = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link\n  \n  # calculate least squares loss function\n  L = crossprod(y - mu)\n}\n\n\n#' #  Obtain Model Estimates\n#'\n#' Setup for use with optim.\n\nX = cbind(1, X)\n\n#' Initial values. Note we'd normally want to handle the sigma differently as\n#' it's bounded by zero, but we'll ignore for demonstration.  Also sigma2 is not\n#' required for the LS approach as it is the objective function.\n\ninit = c(1, rep(0, ncol(X)))\nnames(init) = c('sigma2', 'intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = lm_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptlmLS = optim(\n  par = init[-1],\n  fn  = lm_LS,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\npars_ML = optlmML$par\npars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par)  # calculate sigma2 and add\n\n\n\n#' #  Comparison\n#' \n#' Compare to `lm` which uses QR decomposition.\n\nmodlm = lm(y ~ ., dfXy)\n\n#' Example\n#' \n# QRX = qr(X)\n# Q = qr.Q(QRX)\n# R = qr.R(QRX)\n# Bhat = solve(R) %*% crossprod(Q, y)\n# alternate: qr.coef(QRX, y)\n\nround(\n  rbind(\n    pars_ML,\n    pars_LS,\n    modlm = c(summary(modlm)$sigma^2, coef(modlm))), \n  digits = 3\n)\n\n        sigma2 intercept    b1    b2\npars_ML  0.219    -0.432 0.133 0.112\npars_LS  0.226    -0.432 0.133 0.112\nmodlm    0.226    -0.432 0.133 0.112\n\n#' The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in\n#' the traditional least squares approach; diminishes with increasing N as both\n#' tend toward whatever sd^2 you specify when creating the y response above.\n\n\n#'\n#'Compare to glm, which by default assumes gaussian family with identity link\n#'and uses `lm.fit`.\n#'\nmodglm = glm(y ~ ., data = dfXy)\nsummary(modglm)\n\n\nCall:\nglm(formula = y ~ ., data = dfXy)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.93651  -0.33037  -0.06222   0.31068   1.03991  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.43247    0.04807  -8.997 1.97e-14 ***\nX1           0.13341    0.05243   2.544   0.0125 *  \nX2           0.11191    0.04950   2.261   0.0260 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.2262419)\n\n    Null deviance: 24.444  on 99  degrees of freedom\nResidual deviance: 21.945  on 97  degrees of freedom\nAIC: 140.13\n\nNumber of Fisher Scoring iterations: 2\n\n#' Via normal equations.\ncoefs = solve(t(X) %*% X) %*% t(X) %*% y  # coefficients\n\n#' Compare.\n#' \nsqrt(crossprod(y - X %*% coefs) / (N - k - 1))\n\n          [,1]\n[1,] 0.4756489\n\nsummary(modlm)$sigma\n\n[1] 0.4756489\n\nsqrt(modglm$deviance / modglm$df.residual) \n\n[1] 0.4756489\n\nc(sqrt(pars_ML[1]), sqrt(pars_LS[1]))\n\n   sigma2    sigma2 \n0.4684616 0.4756490 \n\n# rerun by adding 3-4 zeros to the N\n\nThis code uses gradient descent to find the beta0 and beta1 coefficients that minimize the mean squared error (MSE). The learning rate alpha can be adjusted to influence the speed of learning."
  },
  {
    "objectID": "posts/ridge/index.html",
    "href": "posts/ridge/index.html",
    "title": "Ridge",
    "section": "",
    "text": "ridge <- function(w, X, y, lambda = .1) {\n  # X: model matrix; \n  # y: target; \n  # lambda: penalty parameter; \n  # w: the weights/coefficients\n  \n  crossprod(y - X %*% w) + lambda * length(y) * crossprod(w)\n}\n\n\nset.seed(8675309)\nN = 500\np = 10\nX = scale(matrix(rnorm(N * p), ncol = p))\nb = c(.5, -.5, .25, -.25, .125, -.125, rep(0, 4))\ny = scale(X %*% b + rnorm(N, sd = .5))\n\n#' Note, if `lambda=0`, result is the same as  `lm.fit`.\n#' \n#' \nresult_ridge = optim(\n  rep(0, ncol(X)),\n  ridge,\n  X = X,\n  y = y,\n  lambda = .1,\n  method = 'BFGS'\n)\n\n#' Analytical result.\n#' \nresult_ridge2 =  solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y)\n\n#' Alternative with augmented data (note sigma ignored as it equals 1, but otherwise\n#' X/sigma and y/sigma).\n#' \nX2 = rbind(X, diag(sqrt(length(y)*.1), ncol(X)))\ny2 = c(y, rep(0, ncol(X)))\nresult_ridge3 = solve(crossprod(X2)) %*% crossprod(X2, y2)\n\n\n\n\n#' `glmnet` is by default a mixture of ridge and lasso penalties, setting alpha\n#' = 1 reduces to lasso, while alpha=0 would be ridge.\n\n\nlibrary(glmnet)\n\nLe chargement a nécessité le package : Matrix\n\n\nLoaded glmnet 4.1-6\n\nglmnet_res = coef(\n  glmnet(\n    X,\n    y,\n    alpha = 0,\n    lambda = c(10, 1, .1),\n    thresh = 1e-12,\n    intercept = F\n  ), \n  s = .1\n)\n\n#' # Comparison\n\ndata.frame(\n  lm     = coef(lm(y ~ . - 1, data.frame(X))),\n  ridge  = result_ridge$par,\n  ridge2 = result_ridge2,\n  ridge3 = result_ridge3,\n  glmnet = glmnet_res[-1, 1],\n  truth  = b\n)\n\n              lm        ridge       ridge2       ridge3       glmnet  truth\nX1   0.534988063  0.485323748  0.485323748  0.485323748  0.485368766  0.500\nX2  -0.529993422 -0.480742032 -0.480742032 -0.480742032 -0.480786661 -0.500\nX3   0.234376590  0.209412833  0.209412833  0.209412833  0.209435147  0.250\nX4  -0.294350608 -0.268814168 -0.268814168 -0.268814168 -0.268837476 -0.250\nX5   0.126037566  0.114963716  0.114963716  0.114963716  0.114973801  0.125\nX6  -0.159386728 -0.145880488 -0.145880488 -0.145880488 -0.145892837 -0.125\nX7  -0.016718534 -0.021658889 -0.021658889 -0.021658889 -0.021655033  0.000\nX8   0.009894575  0.006956965  0.006956965  0.006956965  0.006959470  0.000\nX9  -0.005441959  0.001392244  0.001392244  0.001392244  0.001386661  0.000\nX10  0.010561128  0.010985385  0.010985385  0.010985385  0.010985102  0.000"
  },
  {
    "objectID": "posts/spline/index.html",
    "href": "posts/spline/index.html",
    "title": "Cubic Splines",
    "section": "",
    "text": "library(tidyverse) # for processing and plotting\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n#' # Create the data\nsize = c(1.42,1.58,1.78,1.99,1.99,1.99,2.13,2.13,2.13,\n         2.32,2.32,2.32,2.32,2.32,2.43,2.43,2.78,2.98,2.98)\n\nwear = c(4.0,4.2,2.5,2.6,2.8,2.4,3.2,2.4,2.6,4.8,2.9,\n         3.8,3.0,2.7,3.1,3.3,3.0,2.8,1.7)\n\nx = size - min(size)\nx = x / max(x)\nd = data.frame(wear, x)\n\n#' Cubic spline function\nrk <- function(x, z) {\n  ((z-0.5)^2 - 1/12) * ((x-0.5)^2 - 1/12)/4 -\n    ((abs(x-z)-0.5)^4 - (abs(x-z)-0.5)^2/2 + 7/240) / 24\n}\n\n#' Generate the model matrix.\nsplX <- function(x, knots) {\n  q = length(knots) + 2                # number of parameters\n  n = length(x)                        # number of observations\n  X = matrix(1, n, q)                  # initialized model matrix\n  X[ ,2]   = x                         # set second column to x\n  X[ ,3:q] = outer(x, knots, FUN = rk) # remaining to cubic spline basis\n  X\n}\n\nsplS <- function(knots) {\n  q = length(knots) + 2\n  S = matrix(0, q, q)                         # initialize matrix\n  S[3:q, 3:q] = outer(knots, knots, FUN = rk) # fill in non-zero part\n  S\n}\n\n#' Matrix square root function. Note that there are various packages with their own.\nmatSqrt <- function(S) {\n  d  = eigen(S, symmetric = T)\n  rS = d$vectors %*% diag(d$values^.5) %*% t(d$vectors)\n  rS\n}\n\n#' Penalized fitting function.\nprsFit <- function(y, x, knots, lambda) {\n  q  = length(knots) + 2    # dimension of basis\n  n  = length(x)            # number of observations\n  Xa = rbind(splX(x, knots), matSqrt(splS(knots))*sqrt(lambda)) # augmented model matrix\n  y[(n+1):(n+q)] = 0        # augment the data vector\n  \n  lm(y ~ Xa - 1) # fit and return penalized regression spline\n}\n\n\n\n#' # Example 1\n\n\n#' Unpenalized\n#' \nknots = 1:4/5\nX = splX(x, knots)      # generate model matrix\nmod1 = lm(wear ~ X - 1) # fit model\n\nxp = 0:100/100 # x values for prediction\nXp = splX(xp, knots) # prediction matrix\n\n\n#' Visualize\n\nggplot(aes(x = x, y = wear), data = data.frame(x, wear)) +\n  geom_point(color = \"#FF5500\") +\n  geom_line(aes(x = xp, y = Xp %*% coef(mod1)),\n            data = data.frame(xp, Xp),\n            color = \"#00AAFF\") +\n  labs(x = 'Scaled Engine size', y  = 'Wear Index') +\n  theme_minimal()\n\n\n\n#' # Example 2\n\n\n# Add penalty lambda\nknots = 1:7/8\nd2 = data.frame(x = xp)\n\nfor (i in c(.1, .01, .001, .0001, .00001, .000001)){\n  # fit penalized regression\n  mod2 = prsFit(\n    y = wear,\n    x = x,\n    knots = knots,\n    lambda = i\n  ) \n  # spline choosing lambda\n  Xp = splX(xp, knots) # matrix to map parameters to fitted values at xp\n  LP = Xp %*% coef(mod2)\n  d2[, paste0('lambda = ', i)] = LP[, 1]\n}\n\n#' Examine\n# head(d2)\n\n#' Visualize via ggplot\nd3 = d2 %>%\n  pivot_longer(cols = -x,\n               names_to  = 'lambda',\n               values_to = 'value') %>% \n  mutate(lambda = fct_inorder(lambda))\n\nggplot(d3) +\n  geom_point(aes(x = x, y = wear), col = '#FF5500', data = d) +\n  geom_line(aes(x = x, y = value), col = \"#00AAFF\") +\n  facet_wrap(~lambda) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "R code to estimate a logistic regression model\n\nset.seed(1235)  # ensures replication\n\n\n# predictors and target\n\nN = 10000 # sample size\nk = 2     # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)\n\n# the linear predictor\nlp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these\n\ny = rbinom(N, size = 1, prob = plogis(lp))\n\ndfXy = data.frame(X, y)\n\n\n\n#' \n#' # Functions \n#' \n#' A maximum likelihood approach.\n\nlogreg_ML = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                # coefficients\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                           # linear predictor\n  mu = plogis(LP)                           # logit link\n  \n  # calculate likelihood\n  L = dbinom(y, size = 1, prob = mu, log = TRUE)         # log likelihood\n  #   L =  y*log(mu) + (1 - y)*log(1-mu)    # alternate log likelihood form\n  \n  -sum(L)                                   # optim by default is minimization, and we want to maximize the likelihood \n  # (see also fnscale in optim.control)\n}\n\n# An equivalent approach via exponential loss function.\n\nlogreg_exp = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n\n  # calculate exponential loss function (convert y to -1:1 from 0:1)\n  L = sum(exp(-ifelse(y, 1, -1) * .5 * LP))\n}\n\n\n#' # Obtain Model Estimates\n#' Setup for use with `optim`.\n\nX = cbind(1, X)\n\n# initial values\n\ninit = rep(0, ncol(X))\nnames(init) = c('intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = logreg_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptglmClass = optim(\n  par = init,\n  fn  = logreg_exp,\n  X   = X,\n  y   = y, \n  control = list(reltol = 1e-15)\n)\n\npars_ML  = optlmML$par\npars_exp = optglmClass$par\n\n\n#' # Comparison\n#' \n#' Compare to `glm`.\n\nmodglm = glm(y ~ ., dfXy, family = binomial)\n\nrbind(\n  pars_ML,\n  pars_exp,\n  pars_GLM = coef(modglm)\n)\n\n          intercept        b1         b2\npars_ML  -0.5117658 0.2378927 0.08019841\npars_exp -0.5114284 0.2368478 0.07907056\npars_GLM -0.5117321 0.2378743 0.08032617"
  },
  {
    "objectID": "posts/LASSO/index.html",
    "href": "posts/LASSO/index.html",
    "title": "Lasso",
    "section": "",
    "text": "lasso <- function(\n  X,                   # model matrix\n  y,                   # target\n  lambda  = .1,        # penalty parameter\n  soft    = TRUE,      # soft vs. hard thresholding\n  tol     = 1e-6,      # tolerance\n  iter    = 100,       # number of max iterations\n  verbose = TRUE       # print out iteration number\n) {\n  \n  # soft thresholding function\n  soft_thresh <- function(a, b) {\n    out = rep(0, length(a))\n    out[a >  b] = a[a > b] - b\n    out[a < -b] = a[a < -b] + b\n    out\n  }\n  \n  w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y)\n  tol_curr = 1\n  J = ncol(X)\n  a = rep(0, J)\n  c_ = rep(0, J)\n  i = 1\n  \n  while (tol < tol_curr && i < iter) {\n    w_old = w \n    a = colSums(X^2)\n    l = length(y)*lambda  # for consistency with glmnet approach\n    c_ = sapply(1:J, function(j)  sum( X[,j] * (y - X[,-j] %*% w_old[-j]) ))\n    if (soft) {\n      for (j in 1:J) {\n        w[j] = soft_thresh(c_[j]/a[j], l/a[j])\n      }\n    }\n    else {\n      w = w_old\n      w[c_< l & c_ > -l] = 0\n    }\n    \n    tol_curr = crossprod(w - w_old)  \n    i = i + 1\n    if (verbose && i%%10 == 0) message(i)\n  }\n  \n  w\n}\n\n#' # Data setup\n#' \n#' \nset.seed(8675309)\nN = 500\np = 10\nX = scale(matrix(rnorm(N*p), ncol=p))\nb = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6))\ny = scale(X %*% b + rnorm(N, sd=.5))\nlambda = .1\n\n\n# debugonce(lasso)\n\n#' Note, if `lambda=0`, result is the same as  `lm.fit`.\n#' \n#' \nresult_soft = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol = 1e-12,\n  soft = TRUE\n)\n\nresult_hard = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol    = 1e-12,\n  soft   = FALSE\n)\n\n\n\n\n#' `glmnet` is by default a mixture of ridge and lasso penalties, setting alpha\n#' = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple\n#' values while only wanting the one set to the same lambda value as above (s).\n\n\nlibrary(glmnet)\n\nLe chargement a nécessité le package : Matrix\n\n\nLoaded glmnet 4.1-6\n\nglmnet_res = coef(\n  glmnet(\n    X,\n    y,\n    alpha  = 1,\n    lambda = c(10, 1, lambda),\n    thresh = 1e-12,\n    intercept = FALSE\n  ),\n  s = lambda\n)\n\nlibrary(lassoshooting)\n\nls_res = lassoshooting(\n  X = X,\n  y = y,\n  lambda = length(y) * lambda,\n  thr = 1e-12\n)\n\n\n#' # Comparison\n\ndata.frame(\n  lm = coef(lm(y ~ . - 1, data.frame(X))),\n  lasso_soft = result_soft,\n  lasso_hard = result_hard,\n  lspack = ls_res$coef,\n  glmnet = glmnet_res[-1, 1],\n  truth  = b\n)\n\n              lm  lasso_soft lasso_hard      lspack      glmnet  truth\nX1   0.534988063  0.43542527  0.5348784  0.43542528  0.43552489  0.500\nX2  -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500\nX3   0.234376590  0.12436834  0.2343207  0.12436835  0.12447920  0.250\nX4  -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250\nX5   0.126037566  0.02036410  0.1260132  0.02036407  0.02047015  0.125\nX6  -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125\nX7  -0.016718534  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX8   0.009894575  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX9  -0.005441959  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX10  0.010561128  0.00000000  0.0000000  0.00000000  0.00000000  0.000"
  },
  {
    "objectID": "posts/Logistic/index.html",
    "href": "posts/Logistic/index.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "R code to estimate a logistic regression model\n\nset.seed(1235)  # ensures replication\n\n\n# predictors and target\n\nN = 10000 # sample size\nk = 2     # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)\n\n# the linear predictor\nlp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these\n\ny = rbinom(N, size = 1, prob = plogis(lp))\n\ndfXy = data.frame(X, y)\n\n\n\n#' \n#' # Functions \n#' \n#' A maximum likelihood approach.\n\nlogreg_ML = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                # coefficients\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                           # linear predictor\n  mu = plogis(LP)                           # logit link\n  \n  # calculate likelihood\n  L = dbinom(y, size = 1, prob = mu, log = TRUE)         # log likelihood\n  #   L =  y*log(mu) + (1 - y)*log(1-mu)    # alternate log likelihood form\n  \n  -sum(L)                                   # optim by default is minimization, and we want to maximize the likelihood \n  # (see also fnscale in optim.control)\n}\n\n# An equivalent approach via exponential loss function.\n\nlogreg_exp = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n\n  # calculate exponential loss function (convert y to -1:1 from 0:1)\n  L = sum(exp(-ifelse(y, 1, -1) * .5 * LP))\n}\n\n\n#' # Obtain Model Estimates\n#' Setup for use with `optim`.\n\nX = cbind(1, X)\n\n# initial values\n\ninit = rep(0, ncol(X))\nnames(init) = c('intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = logreg_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptglmClass = optim(\n  par = init,\n  fn  = logreg_exp,\n  X   = X,\n  y   = y, \n  control = list(reltol = 1e-15)\n)\n\npars_ML  = optlmML$par\npars_exp = optglmClass$par\n\n\n#' # Comparison\n#' \n#' Compare to `glm`.\n\nmodglm = glm(y ~ ., dfXy, family = binomial)\n\nrbind(\n  pars_ML,\n  pars_exp,\n  pars_GLM = coef(modglm)\n)\n\n          intercept        b1         b2\npars_ML  -0.5117658 0.2378927 0.08019841\npars_exp -0.5114284 0.2368478 0.07907056\npars_GLM -0.5117321 0.2378743 0.08032617"
  },
  {
    "objectID": "posts/LR/index.html",
    "href": "posts/LR/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "R code to estimate a linear model\n\nset.seed(123)  # ensures replication\n\n# predictors and response\nN = 100 # sample size\nk = 2   # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)  \ny = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5)  # increasing N will get estimated values closer to these\n\ndfXy = data.frame(X, y)\n\n\n\n#'\n#' # Functions \n#'\n\n#' A maximum likelihood approach.\nlm_ML = function(par, X, y) {\n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta   = par[-1]                             # coefficients\n  sigma2 = par[1]                              # error variance\n  sigma  = sqrt(sigma2)\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link in the glm sense\n  \n  # calculate likelihood\n  L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood\n#   L =  -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu)    # alternate log likelihood form\n\n  -sum(L)                                      # optim by default is minimization, and we want to maximize the likelihood \n                                               # (see also fnscale in optim.control)\n}\n\n# An approach via least squares loss function.\n\nlm_LS = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link\n  \n  # calculate least squares loss function\n  L = crossprod(y - mu)\n}\n\n\n#' #  Obtain Model Estimates\n#'\n#' Setup for use with optim.\n\nX = cbind(1, X)\n\n#' Initial values. Note we'd normally want to handle the sigma differently as\n#' it's bounded by zero, but we'll ignore for demonstration.  Also sigma2 is not\n#' required for the LS approach as it is the objective function.\n\ninit = c(1, rep(0, ncol(X)))\nnames(init) = c('sigma2', 'intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = lm_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptlmLS = optim(\n  par = init[-1],\n  fn  = lm_LS,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\npars_ML = optlmML$par\npars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par)  # calculate sigma2 and add\n\n\n\n#' #  Comparison\n#' \n#' Compare to `lm` which uses QR decomposition.\n\nmodlm = lm(y ~ ., dfXy)\n\n#' Example\n#' \n# QRX = qr(X)\n# Q = qr.Q(QRX)\n# R = qr.R(QRX)\n# Bhat = solve(R) %*% crossprod(Q, y)\n# alternate: qr.coef(QRX, y)\n\nround(\n  rbind(\n    pars_ML,\n    pars_LS,\n    modlm = c(summary(modlm)$sigma^2, coef(modlm))), \n  digits = 3\n)\n\n        sigma2 intercept    b1    b2\npars_ML  0.219    -0.432 0.133 0.112\npars_LS  0.226    -0.432 0.133 0.112\nmodlm    0.226    -0.432 0.133 0.112\n\n#' The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in\n#' the traditional least squares approach; diminishes with increasing N as both\n#' tend toward whatever sd^2 you specify when creating the y response above.\n\n\n#'\n#'Compare to glm, which by default assumes gaussian family with identity link\n#'and uses `lm.fit`.\n#'\nmodglm = glm(y ~ ., data = dfXy)\nsummary(modglm)\n\n\nCall:\nglm(formula = y ~ ., data = dfXy)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.93651  -0.33037  -0.06222   0.31068   1.03991  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.43247    0.04807  -8.997 1.97e-14 ***\nX1           0.13341    0.05243   2.544   0.0125 *  \nX2           0.11191    0.04950   2.261   0.0260 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.2262419)\n\n    Null deviance: 24.444  on 99  degrees of freedom\nResidual deviance: 21.945  on 97  degrees of freedom\nAIC: 140.13\n\nNumber of Fisher Scoring iterations: 2\n\n#' Via normal equations.\ncoefs = solve(t(X) %*% X) %*% t(X) %*% y  # coefficients\n\n#' Compare.\n#' \nsqrt(crossprod(y - X %*% coefs) / (N - k - 1))\n\n          [,1]\n[1,] 0.4756489\n\nsummary(modlm)$sigma\n\n[1] 0.4756489\n\nsqrt(modglm$deviance / modglm$df.residual) \n\n[1] 0.4756489\n\nc(sqrt(pars_ML[1]), sqrt(pars_LS[1]))\n\n   sigma2    sigma2 \n0.4684616 0.4756490 \n\n# rerun by adding 3-4 zeros to the N\n\nThis code uses gradient descent to find the beta0 and beta1 coefficients that minimize the mean squared error (MSE). The learning rate alpha can be adjusted to influence the speed of learning."
  },
  {
    "objectID": "posts/ML/index.html",
    "href": "posts/ML/index.html",
    "title": "Maximum Likelihood",
    "section": "",
    "text": "# for replication\nset.seed(1234)\n\n# create the data\ny = rnorm(1000, mean = 5, sd = 2)\nstarting_values = c(0, 1)\n\n# the log likelihood function\nsimple_ll <- function(mu, sigma, verbose = TRUE) {\n  \n  ll = sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n  \n  if (verbose)\n    message(paste(mu, sigma, ll))\n  \n  -ll\n}\n\n# using optim, and L-BFGS-B so as to constrain sigma to be positive by setting\n# the lower bound at zero\nmlnorm = bbmle::mle2(\n  simple_ll,\n  start  = list(mu = 2, sigma = 1),\n  method = \"L-BFGS-B\",\n  lower  = c(sigma = 0),\n  trace = TRUE\n)\n\n2 1 -7248.14586622016\n\n\n2.001 1 -7245.19956062218\n\n\n1.999 1 -7251.09317181815\n\n\n2 1.001 -7236.50591422399\n\n\n2 0.999 -7259.82279352312\n\n\n2.24505468926297 1.96950925692859 -3049.97198996416\n\n\n2.24605468926297 1.96950925692859 -3049.27560582155\n\n\n2.24405468926297 1.96950925692859 -3050.66863190738\n\n\n2.24505468926297 1.97050925692859 -3049.00497761794\n\n\n2.24505468926297 1.96850925692859 -3050.94099240168\n\n\n2.31318028766927 2.05892317065501 -2928.01832087456\n\n\n2.31418028766927 2.05892317065501 -2927.39717837487\n\n\n2.31218028766927 2.05892317065501 -2928.63969926979\n\n\n2.31318028766927 2.05992317065501 -2927.25473617666\n\n\n2.31318028766927 2.05792317065501 -2928.78349111668\n\n\n2.77278505411265 2.432558187808 -2543.10404261981\n\n\n2.77378505411265 2.432558187808 -2542.73672879512\n\n\n2.77178505411265 2.432558187808 -2543.47152543937\n\n\n2.77278505411265 2.433558187808 -2542.91093588634\n\n\n2.77278505411265 2.431558187808 -2543.29772585115\n\n\n3.28615824369724 2.62659417141057 -2372.55987588605\n\n\n3.28715824369724 2.62659417141057 -2372.31923983875\n\n\n3.28515824369724 2.62659417141057 -2372.80065688196\n\n\n3.28615824369724 2.62759417141057 -2372.56920357348\n\n\n3.28615824369724 2.62559417141057 -2372.55082760165\n\n\n4.13268564260831 2.69838754293375 -2230.04880908758\n\n\n4.13368564260831 2.69838754293375 -2229.93706798682\n\n\n4.13168564260831 2.69838754293375 -2230.16068752655\n\n\n4.13268564260831 2.69938754293375 -2230.18342905799\n\n\n4.13268564260831 2.69738754293375 -2229.91431419597\n\n\n4.97473678705946 2.51054560600106 -2154.81488384444\n\n\n4.97573678705946 2.51054560600106 -2154.81939469868\n\n\n4.97373678705946 2.51054560600106 -2154.81053164886\n\n\n4.97473678705946 2.51154560600106 -2154.9620331563\n\n\n4.97473678705946 2.50954560600106 -2154.66787609703\n\n\n5.21489320377246 2.18947573207081 -2124.66864416274\n\n\n5.21589320377246 2.18947573207081 -2124.72467223707\n\n\n5.21389320377246 2.18947573207081 -2124.61282469102\n\n\n5.21489320377246 2.19047573207081 -2124.73999169529\n\n\n5.21489320377246 2.18847573207081 -2124.59761629464\n\n\n5.00995988321629 1.92622586907284 -2110.66941028985\n\n\n5.01095988321629 1.92622586907284 -2110.68656617869\n\n\n5.00895988321629 1.92622586907284 -2110.65252391764\n\n\n5.00995988321629 1.92722586907284 -2110.6321552604\n\n\n5.00995988321629 1.92522586907284 -2110.70726284067\n\n\n4.91335420165115 2.01962213907747 -2109.22254566271\n\n\n4.91435420165115 2.01962213907747 -2109.21446710973\n\n\n4.91235420165115 2.01962213907747 -2109.23086938141\n\n\n4.91335420165115 2.02062213907747 -2109.23528509214\n\n\n4.91335420165115 2.01862213907747 -2109.21027799159\n\n\n4.94651143156101 1.99717266850308 -2108.9227147788\n\n\n4.94751143156101 1.99717266850308 -2108.922766383\n\n\n4.94551143156101 1.99717266850308 -2108.92291388295\n\n\n4.94651143156101 1.99817266850308 -2108.9247147721\n\n\n4.94651143156101 1.99617266850308 -2108.92121357253\n\n\n4.94623334002317 1.9930802530799 -2108.91977119769\n\n\n4.94723334002317 1.9930802530799 -2108.91975300755\n\n\n4.94523334002317 1.9930802530799 -2108.92004112679\n\n\n4.94623334002317 1.9940802530799 -2108.91972198122\n\n\n4.94623334002317 1.9920802530799 -2108.92032434539\n\n\n4.94687250066485 1.99367860362636 -2108.91964064385\n\n\n4.94787250066485 1.99367860362636 -2108.91978326969\n\n\n4.94587250066485 1.99367860362636 -2108.91974960589\n\n\n4.94687250066485 1.99467860362636 -2108.91989239182\n\n\n4.94687250066485 1.99267860362636 -2108.91989207136\n\n\n4.94680327041371 1.99367954739935 -2108.91964008206\n\n\n4.94780327041371 1.99367954739935 -2108.91976529029\n\n\n4.94580327041371 1.99367954739935 -2108.91976646146\n\n\n4.94680327041371 1.99467954739935 -2108.91989230487\n\n\n4.94680327041371 1.99267954739935 -2108.91989103353\n\n\n4.94680327041371 1.99367954739935 -2108.91964008206\n4.94680327041371 1.99367954739935 -2108.91964008206\n\n\n5.44148359745509 1.99367954739935 -2139.70218265137\n\n\n4.45212294337234 1.99367954739935 -2139.70276200931\n\n\n5.1941434339344 1.99367954739935 -2116.61520330464\n\n\n4.69946310689303 1.99367954739935 -2116.61549298362\n\n\n5.07047335217406 1.99367954739935 -2110.84349467783\n\n\n4.82313318865337 1.99367954739935 -2110.84363951732\n\n\n5.00863831129389 1.99367954739935 -2109.40058562607\n\n\n4.88496822953354 1.99367954739935 -2109.40065804581\n\n\n4.94680327041371 2.19304750213929 -2117.45310674386\n\n\n4.94680327041371 1.79431159265942 -2120.84287722532\n\n\n4.94680327041371 2.09336352476932 -2111.22462188457\n\n\n4.94680327041371 1.89399557002938 -2111.64287508635\n\n\n4.94680327041371 2.04352153608433 -2109.51949140477\n\n\n4.94680327041371 1.94383755871437 -2109.57154503504\n\n\n4.94680327041371 2.01860054174184 -2109.07270965763\n\n\n4.94680327041371 1.96875855305686 -2109.07917960438\n\n\n5.44148359745509 2.19304750213929 -2142.89322456974\n\n\n4.45212294337234 1.79431159265942 -2158.8467314565\n\n\n5.1941434339344 2.09336352476932 -2118.20472458079\n\n\n4.69946310689303 1.89399557002938 -2120.1701359191\n\n\n5.07047335217406 2.04352153608433 -2111.35064390521\n\n\n4.82313318865337 1.94383755871437 -2111.5954760649\n\n\n5.00863831129389 2.01860054174184 -2109.54185330672\n\n\n4.88496822953354 1.96875855305686 -2109.57245231197\n\n\n4.94680327041371 1.99367954739935 -2108.91964008206\n\n\n4.94729795074076 1.99367954739935 -2108.91967057521\n\n\n4.94630859008667 1.99367954739935 -2108.91967115457\n\n\n4.94680327041371 1.99387891535409 -2108.91965024903\n\n\n4.94680327041371 1.99348017944461 -2108.91964991504\n\n\n4.94705061057723 1.99367954739935 -2108.91964763293\n\n\n4.94655593025019 1.99367954739935 -2108.91964792261\n\n\n4.94680327041371 1.99377923137672 -2108.91964266618\n\n\n4.94680327041371 1.99357986342198 -2108.91964249793\n\n\n4.94692694049547 1.99367954739935 -2108.91964193357\n\n\n4.94667960033195 1.99367954739935 -2108.91964207841\n\n\n4.94680327041371 1.99372938938804 -2108.9196407492\n\n\n4.94680327041371 1.99362970541067 -2108.91964066492\n\n\n4.9468651054546 1.99367954739935 -2108.91964052683\n\n\n4.94674143537283 1.99367954739935 -2108.91964059925\n\n\n4.94680327041371 1.99370446839369 -2108.91964025939\n\n\n4.94680327041371 1.99365462640501 -2108.91964021723\n\nmlnorm\n\n\nCall:\nbbmle::mle2(minuslogl = simple_ll, start = list(mu = 2, sigma = 1), \n    method = \"L-BFGS-B\", trace = TRUE, lower = c(sigma = 0))\n\nCoefficients:\n      mu    sigma \n4.946803 1.993680 \n\nLog-likelihood: -2108.92 \n\n# compare to an intercept only regression model\nsummary(lm(y~1))\n\n\nCall:\nlm(formula = y ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7389 -1.2933 -0.0264  1.2848  6.4450 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.94681    0.06308   78.42   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.995 on 999 degrees of freedom\n\n# for replication\nset.seed(1234)\n\n# predictor\nX = rnorm(1000)\n\n# coefficients for intercept and predictor\nbeta = c(5, 2)\n\n# add intercept to X and create y with some noise\ny = cbind(1, X) %*% beta + rnorm(1000, sd = 2.5)\n\nregression_ll <- function(sigma = 1, Int = 0, b1 = 0) {\n  coefs = c(Int, b1)\n  \n  mu = cbind(1,X)%*%coefs\n  \n  ll = -sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n  \n  message(paste(sigma, Int, b1, ll))\n  \n  ll\n}\n\nmlopt =  bbmle::mle2(regression_ll, method = \"L-BFGS-B\", lower = c(sigma = 0)) \n\n1 0 0 18604.1981758075\n\n\n1.001 0 0 18569.8801419815\n\n\n0.999 0 0 18638.6213213676\n\n\n1 0.001 0 18599.2155996033\n\n\n1 0 0 18604.1981758075\n\n\n1 0 0.001 18602.2054290637\n\n\n1 0 0 18604.1981758075\n\n\n1.9880300529133 0.143231031955916 0.0572842563449178 5874.27437077878\n\n\n1.9890300529133 0.143231031955916 0.0572842563449178 5870.48660201951\n\n\n1.9870300529133 0.143231031955916 0.0572842563449178 5878.06836613716\n\n\n1.9880300529133 0.144231031955916 0.0572842563449178 5873.04953625452\n\n\n1.9880300529133 0.142231031955916 0.0572842563449178 5875.49945832261\n\n\n1.9880300529133 0.143231031955916 0.0582842563449178 5873.78361577242\n\n\n1.9880300529133 0.143231031955916 0.0562842563449178 5874.7653773866\n\n\n2.11110658017379 0.185766269496337 0.0743312519780744 5397.7865967707\n\n\n2.11210658017379 0.185766269496337 0.0743312519780744 5394.72743863563\n\n\n2.11010658017379 0.185766269496337 0.0743312519780744 5400.85055430495\n\n\n2.11110658017379 0.186766269496337 0.0743312519780744 5396.70985602148\n\n\n2.11110658017379 0.184766269496337 0.0743312519780744 5398.86356189761\n\n\n2.11110658017379 0.185766269496337 0.0753312519780744 5397.35494502904\n\n\n2.11110658017379 0.185766269496337 0.0733312519780744 5398.21847163247\n\n\n2.63883382213357 0.448124289511769 0.17969362765884 4073.30544842278\n\n\n2.63983382213357 0.448124289511769 0.17969362765884 4072.02997311015\n\n\n2.63783382213357 0.448124289511769 0.17969362765884 4074.5826619819\n\n\n2.63883382213357 0.449124289511769 0.17969362765884 4072.65358257869\n\n\n2.63883382213357 0.447124289511769 0.17969362765884 4073.95745787396\n\n\n2.63883382213357 0.448124289511769 0.18069362765884 4073.04322485671\n\n\n2.63883382213357 0.448124289511769 0.17869362765884 4073.56781479106\n\n\n3.04089183158435 0.7613343171553 0.305922193305561 3503.11416657998\n\n\n3.04189183158435 0.7613343171553 0.305922193305561 3502.47528743065\n\n\n3.03989183158435 0.7613343171553 0.305922193305561 3503.75389272083\n\n\n3.04089183158435 0.7623343171553 0.305922193305561 3502.65678927044\n\n\n3.04089183158435 0.7603343171553 0.305922193305561 3503.57165203243\n\n\n3.04089183158435 0.7613343171553 0.306922193305561 3502.92937321464\n\n\n3.04089183158435 0.7613343171553 0.304922193305561 3503.29906748211\n\n\n3.50548673156963 1.32656376978785 0.534912398361948 3069.39135271974\n\n\n3.50648673156963 1.32656376978785 0.534912398361948 3069.16552876523\n\n\n3.50448673156963 1.32656376978785 0.534912398361948 3069.6175328417\n\n\n3.50548673156963 1.32756376978785 0.534912398361948 3069.09267848316\n\n\n3.50548673156963 1.32556376978785 0.534912398361948 3069.69010833363\n\n\n3.50548673156963 1.32656376978785 0.535912398361948 3069.26960287954\n\n\n3.50548673156963 1.32656376978785 0.533912398361948 3069.51318348116\n\n\n3.87501696536827 2.13425733522368 0.86492142510714 2801.36548299892\n\n\n3.87601696536827 2.13425733522368 0.86492142510714 2801.35116711562\n\n\n3.87401696536827 2.13425733522368 0.86492142510714 2801.37994321472\n\n\n3.87501696536827 2.13525733522368 0.86492142510714 2801.17426230608\n\n\n3.87501696536827 2.13325733522368 0.86492142510714 2801.55677028847\n\n\n3.87501696536827 2.13425733522368 0.86592142510714 2801.28627032369\n\n\n3.87501696536827 2.13425733522368 0.86392142510714 2801.4447618976\n\n\n4.0541874807185 3.31772092855976 1.355047412012 2607.61146017698\n\n\n4.0551874807185 3.31772092855976 1.355047412012 2607.71561055239\n\n\n4.0531874807185 3.31772092855976 1.355047412012 2607.50735443021\n\n\n4.0541874807185 3.31872092855976 1.355047412012 2607.50797693215\n\n\n4.0541874807185 3.31672092855976 1.355047412012 2607.71500426224\n\n\n4.0541874807185 3.31772092855976 1.356047412012 2607.56683151831\n\n\n4.0541874807185 3.31772092855976 1.354047412012 2607.65614933509\n\n\n3.83578971002779 4.79796763903706 1.98369021762435 2469.67379037265\n\n\n3.83678971002779 4.79796763903706 1.98369021762435 2469.82690386735\n\n\n3.83478971002779 4.79796763903706 1.98369021762435 2469.52069306467\n\n\n3.83578971002779 4.79896763903706 1.98369021762435 2469.65765738742\n\n\n3.83578971002779 4.79696763903706 1.98369021762435 2469.68999132369\n\n\n3.83578971002779 4.79796763903706 1.98469021762435 2469.6637459055\n\n\n3.83578971002779 4.79796763903706 1.98269021762435 2469.68390242468\n\n\n3.1597405623082 5.73530101810177 2.41580240215036 2397.00939282354\n\n\n3.1607405623082 5.73530101810177 2.41580240215036 2397.11857609438\n\n\n3.1587405623082 5.73530101810177 2.41580240215036 2396.9003062564\n\n\n3.1597405623082 5.73630101810177 2.41580240215036 2397.07835055202\n\n\n3.1597405623082 5.73430101810177 2.41580240215036 2396.94053525572\n\n\n3.1597405623082 5.73530101810177 2.41680240215036 2397.03513138541\n\n\n3.1597405623082 5.73530101810177 2.41480240215036 2396.98375386095\n\n\n2.60769965863193 5.46205588723569 2.333699479299 2333.51800275284\n\n\n2.60869965863193 5.46205588723569 2.333699479299 2333.55179145289\n\n\n2.60669965863193 5.46205588723569 2.333699479299 2333.48446944127\n\n\n2.60769965863193 5.46305588723569 2.333699479299 2333.57938566806\n\n\n2.60769965863193 5.46105588723569 2.333699479299 2333.45676689434\n\n\n2.60769965863193 5.46205588723569 2.334699479299 2333.54485495317\n\n\n2.60769965863193 5.46205588723569 2.332699479299 2333.49129678502\n\n\n2.20908317935263 4.41732563868732 1.8843877657646 2370.90331720084\n\n\n2.21008317935263 4.41732563868732 1.8843877657646 2370.75931820755\n\n\n2.20808317935263 4.41732563868732 1.8843877657646 2371.0479219923\n\n\n2.20908317935263 4.41832563868732 1.8843877657646 2370.7772180429\n\n\n2.20908317935263 4.41632563868732 1.8843877657646 2371.02962127476\n\n\n2.20908317935263 4.41732563868732 1.8853877657646 2370.85487322291\n\n\n2.20908317935263 4.41732563868732 1.8833877657646 2370.95196494626\n\n\n2.45550036900126 5.06315817489771 2.16214379312588 2314.23172079198\n\n\n2.45650036900126 5.06315817489771 2.16214379312588 2314.23435895171\n\n\n2.45450036900126 5.06315817489771 2.16214379312588 2314.2294113133\n\n\n2.45550036900126 5.06415817489771 2.16214379312588 2314.23554782014\n\n\n2.45550036900126 5.06215817489771 2.16214379312588 2314.22805961554\n\n\n2.45550036900126 5.06315817489771 2.16314379312588 2314.23547119379\n\n\n2.45550036900126 5.06315817489771 2.16114379312588 2314.22813531234\n\n\n2.44340650845624 5.03664444038764 2.13636232105316 2314.14146131506\n\n\n2.44440650845624 5.03664444038764 2.13636232105316 2314.14014827181\n\n\n2.44240650845624 5.03664444038764 2.13636232105316 2314.14311117237\n\n\n2.44340650845624 5.03764444038764 2.13636232105316 2314.14100019054\n\n\n2.44340650845624 5.03564444038764 2.13636232105316 2314.14208993715\n\n\n2.44340650845624 5.03664444038764 2.13736232105316 2314.14107292127\n\n\n2.44340650845624 5.03664444038764 2.13536232105316 2314.14201626764\n\n\n2.45028282535464 5.03866993984521 2.13844911087289 2314.13780826926\n\n\n2.45128282535464 5.03866993984521 2.13844911087289 2314.13879385892\n\n\n2.44928282535464 5.03866993984521 2.13844911087289 2314.13715479378\n\n\n2.45028282535464 5.03966993984521 2.13844911087289 2314.13767784953\n\n\n2.45028282535464 5.03766993984521 2.13844911087289 2314.13810524778\n\n\n2.45028282535464 5.03866993984521 2.13944911087289 2314.13775870453\n\n\n2.45028282535464 5.03866993984521 2.13744911087289 2314.13802345927\n\n\n2.44762453680172 5.04073025686143 2.1399465597729 2314.13669176168\n\n\n2.44862453680172 5.04073025686143 2.1399465597729 2314.13679309145\n\n\n2.44662453680172 5.04073025686143 2.1399465597729 2314.13692435388\n\n\n2.44762453680172 5.04173025686143 2.1399465597729 2314.13689832011\n\n\n2.44762453680172 5.03973025686143 2.1399465597729 2314.13665212402\n\n\n2.44762453680172 5.04073025686143 2.1409465597729 2314.13688149659\n\n\n2.44762453680172 5.04073025686143 2.1389465597729 2314.13666801199\n\n\n2.44780022941175 5.03993567558159 2.13925736768767 2314.13660372081\n\n\n2.44880022941175 5.03993567558159 2.13925736768767 2314.1367637145\n\n\n2.44680022941175 5.03993567558159 2.13925736768767 2314.13677752921\n\n\n2.44780022941175 5.04093567558159 2.13925736768767 2314.13668069582\n\n\n2.44780022941175 5.03893567558159 2.13925736768767 2314.13669364261\n\n\n2.44780022941175 5.03993567558159 2.14025736768767 2314.13668257634\n\n\n2.44780022941175 5.03993567558159 2.13825736768767 2314.13669082668\n\n\n2.44782290067489 5.03997575794523 2.13928362353337 2314.13660347139\n\n\n2.44882290067489 5.03997575794523 2.13928362353337 2314.13677102504\n\n\n2.44682290067489 5.03997575794523 2.13928362353337 2314.13676970437\n\n\n2.44782290067489 5.04097575794523 2.13928362353337 2314.13668701793\n\n\n2.44782290067489 5.03897575794523 2.13928362353337 2314.13668681859\n\n\n2.44782290067489 5.03997575794523 2.14028362353337 2314.13668650492\n\n\n2.44782290067489 5.03997575794523 2.13828362353337 2314.1366863962\n\n\n2.44782290067489 5.03997575794523 2.13928362353337 2314.13660347139\n2.44782290067489 5.03997575794523 2.13928362353337 2314.13660347139\n\n\n2.69260519074238 5.03997575794523 2.13928362353337 2322.67008817396\n\n\n2.2030406106074 5.03997575794523 2.13928362353337 2326.05981623152\n\n\n2.57021404570863 5.03997575794523 2.13928362353337 2316.44159493811\n\n\n2.32543175564115 5.03997575794523 2.13928362353337 2316.85982724572\n\n\n2.50901847319176 5.03997575794523 2.13928362353337 2314.73645980295\n\n\n2.38662732815802 5.03997575794523 2.13928362353337 2314.78850302531\n\n\n2.47842068693333 5.03997575794523 2.13928362353337 2314.28967559777\n\n\n2.41722511441645 5.03997575794523 2.13928362353337 2314.29614034544\n\n\n2.44782290067489 5.54397333373976 2.13928362353337 2335.33328825995\n\n\n2.44782290067489 4.53597818215071 2.13928362353337 2335.33318779151\n\n\n2.44782290067489 5.29197454584249 2.13928362353337 2319.43578722709\n\n\n2.44782290067489 4.78797697004797 2.13928362353337 2319.43573699287\n\n\n2.44782290067489 5.16597515189386 2.13928362353337 2315.46140568959\n\n\n2.44782290067489 4.9139763639966 2.13928362353337 2315.46138057248\n\n\n2.44782290067489 5.10297545491955 2.13928362353337 2314.46780716558\n\n\n2.44782290067489 4.97697606097092 2.13928362353337 2314.46779460703\n\n\n2.44782290067489 5.03997575794523 2.3532119858867 2317.93418520727\n\n\n2.44782290067489 5.03997575794523 1.92535526118003 2317.93416194984\n\n\n2.44782290067489 5.03997575794523 2.24624780471003 2315.08600181254\n\n\n2.44782290067489 5.03997575794523 2.0323194423567 2315.08599018383\n\n\n2.44782290067489 5.03997575794523 2.1927657141217 2314.37395451027\n\n\n2.44782290067489 5.03997575794523 2.08580153294503 2314.37394869591\n\n\n2.44782290067489 5.03997575794523 2.16602466882753 2314.19594195791\n\n\n2.44782290067489 5.03997575794523 2.1125425782392 2314.19593905073\n\n\n2.69260519074238 5.54397333373976 2.13928362353337 2340.18800948682\n\n\n2.2030406106074 4.53597818215071 2.13928362353337 2352.22843884896\n\n\n2.57021404570863 5.29197454584249 2.13928362353337 2321.24811081629\n\n\n2.32543175564115 4.78797697004797 2.13928362353337 2322.731443336\n\n\n2.50901847319176 5.16597515189386 2.13928362353337 2315.99742562634\n\n\n2.38662732815802 4.9139763639966 2.13928362353337 2316.18208840449\n\n\n2.47842068693333 5.10297545491955 2.13928362353337 2314.61275190364\n\n\n2.41722511441645 4.97697606097092 2.13928362353337 2314.63576913378\n\n\n2.69260519074238 5.03997575794523 2.3532119858867 2325.80858547634\n\n\n2.2030406106074 5.03997575794523 1.92535526118003 2330.74816003208\n\n\n2.57021404570863 5.03997575794523 2.24624780471003 2317.30272722033\n\n\n2.32543175564115 5.03997575794523 2.0323194423567 2317.91177928166\n\n\n2.50901847319176 5.03997575794523 2.1927657141217 2314.96237393871\n\n\n2.38662732815802 5.03997575794523 2.08580153294503 2315.03817584531\n\n\n2.47842068693333 5.03997575794523 2.16602466882753 2314.34755798059\n\n\n2.41722511441645 5.03997575794523 2.1125425782392 2314.35698759872\n\n\n2.44782290067489 5.54397333373976 2.3532119858867 2338.65226992061\n\n\n2.44782290067489 4.53597818215071 1.92535526118003 2338.65214619474\n\n\n2.44782290067489 5.29197454584249 2.24624780471003 2320.26553554943\n\n\n2.44782290067489 4.78797697004797 2.0323194423567 2320.2654736865\n\n\n2.44782290067489 5.16597515189386 2.1927657141217 2315.66884422377\n\n\n2.44782290067489 4.9139763639966 2.08580153294503 2315.6688132923\n\n\n2.44782290067489 5.10297545491955 2.16602466882753 2314.51966752592\n\n\n2.44782290067489 4.97697606097092 2.1125425782392 2314.51965206019\n\n\n2.44782290067489 5.03997575794523 2.13928362353337 2314.13660347139\n\n\n2.44806768296496 5.03997575794523 2.13928362353337 2314.13661365915\n\n\n2.44757811838482 5.03997575794523 2.13928362353337 2314.13661328358\n\n\n2.44782290067489 5.04047975552103 2.13928362353337 2314.13662471826\n\n\n2.44782290067489 5.03947176036944 2.13928362353337 2314.13662461779\n\n\n2.44782290067489 5.03997575794523 2.13949755189572 2314.13660728059\n\n\n2.44782290067489 5.03997575794523 2.13906969517101 2314.13660725733\n\n\n2.44794529181992 5.03997575794523 2.13928362353337 2314.1366060659\n\n\n2.44770050952986 5.03997575794523 2.13928362353337 2314.13660587687\n\n\n2.44782290067489 5.04022775673313 2.13928362353337 2314.13660879567\n\n\n2.44782290067489 5.03972375915734 2.13928362353337 2314.13660874543\n\n\n2.44782290067489 5.03997575794523 2.13939058771454 2314.1366044266\n\n\n2.44782290067489 5.03997575794523 2.13917665935219 2314.13660441497\n\n\n2.44788409624741 5.03997575794523 2.13928362353337 2314.13660414373\n\n\n2.44776170510237 5.03997575794523 2.13928362353337 2314.13660404905\n\n\n2.44782290067489 5.04010175733918 2.13928362353337 2314.13660480874\n\n\n2.44782290067489 5.03984975855128 2.13928362353337 2314.13660478362\n\n\n2.44782290067489 5.03997575794523 2.13933710562395 2314.13660371165\n\n\n2.44782290067489 5.03997575794523 2.13923014144278 2314.13660370583\n\n\n2.44785349846115 5.03997575794523 2.13928362353337 2314.13660365132\n\n\n2.44779230288863 5.03997575794523 2.13928362353337 2314.13660360396\n\n\n2.44782290067489 5.04003875764221 2.13928362353337 2314.13660380887\n\n\n2.44782290067489 5.03991275824826 2.13928362353337 2314.13660379631\n\n\n2.44782290067489 5.03997575794523 2.13931036457866 2314.13660353218\n\n\n2.44782290067489 5.03997575794523 2.13925688248807 2314.13660352928\n\nsummary(mlopt)\n\nLength  Class   Mode \n     1   mle2     S4 \n\n# plot(profile(mlopt), absVal=F)\n\nmodlm = lm(y ~ X)\nsummary(modlm)\n\n\nCall:\nlm(formula = y ~ X)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.9152 -1.6097  0.0363  1.6343  7.6711 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  5.03998    0.07751   65.02   <2e-16 ***\nX            2.13928    0.07773   27.52   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.45 on 998 degrees of freedom\nMultiple R-squared:  0.4315,    Adjusted R-squared:  0.4309 \nF-statistic: 757.5 on 1 and 998 DF,  p-value: < 2.2e-16\n\n- 2 * logLik(modlm)\n\n'log Lik.' 4628.273 (df=3)"
  },
  {
    "objectID": "posts/GD/index.html",
    "href": "posts/GD/index.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "set.seed(8675309)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)  # model matrix\n\n\n\n#' # Gradient Descent Algorithm\n\n\ngd = function(\n  par,\n  X,\n  y,\n  tolerance = 1e-3,\n  maxit     = 1000,\n  stepsize  = 1e-3,\n  adapt     = FALSE,\n  verbose   = TRUE,\n  plotLoss  = TRUE\n  ) {\n  \n  # initialize\n  beta = par; names(beta) = colnames(X)\n  loss = crossprod(X %*% beta - y)\n  tol  = 1\n  iter = 1\n  \n  while(tol > tolerance && iter < maxit){\n    \n    LP   = X %*% beta\n    grad = t(X) %*% (LP - y)\n    betaCurrent = beta - stepsize * grad\n    tol  = max(abs(betaCurrent - beta))\n    beta = betaCurrent\n    loss = append(loss, crossprod(LP - y))\n    iter = iter + 1\n    \n    if (adapt)\n      stepsize = ifelse(\n        loss[iter] < loss[iter - 1],  \n        stepsize * 1.2, \n        stepsize * .8\n      )\n    \n    if (verbose && iter %% 10 == 0)\n      message(paste('Iteration:', iter))\n  }\n  \n  if (plotLoss)\n    plot(loss, type = 'l', bty = 'n')\n  \n  list(\n    par    = beta,\n    loss   = loss,\n    RSE    = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), \n    iter   = iter,\n    fitted = LP\n  )\n}\n\n\n#' ## Run\n#' \n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you'd have to fiddle with the `stepsize`, which could \n#' be assessed via cross-validation, or alternatively one can use an\n#' adaptive approach, a simple one of which is implemented in this function.\n\ngd_result = gd(\n  init,\n  X = X,\n  y = y,\n  tolerance = 1e-8,\n  stepsize  = 1e-4,\n  adapt     = TRUE\n)\n\nIteration: 10\n\n\nIteration: 20\n\n\nIteration: 30\n\n\nIteration: 40\n\n\nIteration: 50\n\n\nIteration: 60\n\n\nIteration: 70\n\n\n\n\nstr(gd_result)\n\nList of 5\n $ par   : num [1:3, 1] 0.985 0.487 0.218\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:3] \"Intercept\" \"x1\" \"x2\"\n  .. ..$ : NULL\n $ loss  : num [1:70] 2315 2315 2075 1918 1760 ...\n $ RSE   : num [1, 1] 1.03\n $ iter  : num 70\n $ fitted: num [1:1000, 1] 0.441 1.061 0.43 2.125 1.858 ...\n\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n\nrbind(\n  gd = round(gd_result$par[, 1], 5),\n  lm = coef(lm(y ~ x1 + x2))\n)\n\n   Intercept        x1        x2\ngd 0.9847800 0.4867900 0.2175200\nlm 0.9847803 0.4867896 0.2175169"
  },
  {
    "objectID": "posts/GP/index.html",
    "href": "posts/GP/index.html",
    "title": "Gaussian Process",
    "section": "",
    "text": "muFn = function(x){\n  x = sapply(x, function(x) x=0)\n  x\n}\n\n# The covariance function; here it is the squared exponential kernel.\n# l is the horizontal scale, sigmaf is the vertical scale.\n# See ?covSEiso in the gpr package for example, which is also based on Rasmussen and\n# Williams Matlab code (gpml Matlab library)\n\nKfn = function(x, l=1, sigmaf=1){\n    sigmaf * exp( -(1/(2*l^2)) * as.matrix(dist(x, upper=T, diag=T)^2) )\n}\n\n\n#####################\n### Preliminaries ###\n#####################\n\nl = 1           # for l, sigmaf, see note at covariance function\nsigmaf = 1      \nkeps = 1e-8     # see note at Kstarstar\nnprior = 5      # number of prior draws\nnpostpred = 3   # number of posterior predictive draws\n\n##################\n### Prior plot ###\n##################\n\n# data setup\nrequire(MASS)\n\nLe chargement a nécessité le package : MASS\n\nxg1 = seq(-5, 5, .2)\nyg1 = mvrnorm(nprior,\n              mu = muFn(xg1),\n              Sigma = Kfn(xg1, l = l, sigmaf = sigmaf))\n\n# plot prior\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# reshape data for plotting\ngdat = melt(data.frame(\n  x = xg1,\n  y = t(yg1),\n  sd = apply(yg1, 2, sd)\n), id = c('x', 'sd'))\n# head(gdat) # inspect if desired\n\ng1 = ggplot(aes(x = x, y = value), data = gdat) +\n  geom_line(aes(group = variable), color = '#FF5500', alpha = .5) +\n  labs(title = 'Prior') +\n  theme_minimal()\n\n# g1\n\n#########################################\n### generate noise-less training data ###\n#########################################\nXtrain = c(-4, -3, -2, -1, 1)\nytrain = sin(Xtrain)\nnTrain = length(Xtrain)\n\nXtest = seq(-5, 5, .2)\nnTest = length(Xtest)\n\n#####################################\n### generate posterior predictive ###\n#####################################\n\n# Create K, K*, and K** matrices as defined in the texts\nK = Kfn(Xtrain, l=l, sigmaf=sigmaf)  \nK_ = Kfn(c(Xtrain, Xtest), l=l, sigmaf=sigmaf)                                 # initial matrix\nKstar = K_[1:nTrain, (nTrain+1):ncol(K_)]                                      # dim = N x N*\ntKstar = t(Kstar)                                                              # dim = N* x N\nKstarstar = K_[(nTrain+1):nrow(K_), (nTrain+1):ncol(K_)] + keps*diag(nTest)    # dim = N* x N*; the keps part is for positive definiteness\nKinv = solve(K)\n\n# calculate posterior mean and covariance\npostMu = muFn(Xtest) + t(Kstar) %*% Kinv %*% (ytrain-muFn(Xtrain))\npostCov = Kstarstar - t(Kstar) %*% Kinv %*% Kstar\ns2 = diag(postCov)\n# R = chol(postCov)  \n# L = t(R)      # L is used in alternative formulation below based on gaussSample.m\n\n# generate draws from posterior predictive\ny2 = data.frame(t(mvrnorm(npostpred, mu=postMu, Sigma=postCov)))\n# y2 = data.frame(replicate(npostpred, postMu + L %*% rnorm(postMu))) # alternative\n\n#################################\n### Posterior predictive plot ###\n#################################\n\n# reshape data for plotting\ngdat = melt(data.frame(x=Xtest, y=y2, selower=postMu-2*sqrt(s2), seupper=postMu+2*sqrt(s2)),\n            id=c('x', 'selower', 'seupper'))\n\ng2 = ggplot(aes(x=x, y=value), data=gdat) + \n  geom_ribbon(aes(ymin=selower, ymax=seupper,group=variable), fill='gray90') +\n  geom_line(aes(group=variable), color='#FF5500') +\n  geom_point(aes(x=Xtrain, y=ytrain), data=data.frame(Xtrain, ytrain)) +\n  labs(title='Posterior Predictive') +\n  theme_minimal()\n\n# g2\n\n####################################################\n### Plot prior and posterior predictive together ###\n####################################################\n\nlibrary(gridExtra)\ngrid.arrange(g1, g2, ncol=2)"
  },
  {
    "objectID": "posts/BNN/index.html",
    "href": "posts/BNN/index.html",
    "title": "Basic Neural Network",
    "section": "",
    "text": "X = matrix( \n  c(0, 0, 1, \n    0, 1, 1, \n    1, 0, 1, \n    1, 1, 1),\n  nrow  = 4,\n  ncol  = 3,\n  byrow = TRUE\n)\n\n# output dataset            \ny = c(0, 0, 1, 1)\n\n# seed random numbers to make calculation\n# deterministic (just a good practice)\nset.seed(1)\n\n# initialize weights randomly with mean 0\nsynapse_0 = matrix(runif(3, min = -1, max = 1), 3, 1)\n\n# sigmoid function\nnonlin <- function(x, deriv = FALSE) {\n  if (deriv)\n    x * (1 - x)\n  else\n    plogis(x)\n}\n\n\nnn_1 <- function(X, y, synapse_0, maxiter = 10000) {\n  \n  for (iter in 1:maxiter) {\n  \n      # forward propagation\n      layer_0 = X\n      layer_1 = nonlin(layer_0 %*% synapse_0)\n  \n      # how much did we miss?\n      layer_1_error = y - layer_1\n  \n      # multiply how much we missed by the \n      # slope of the sigmoid at the values in layer_1\n      l1_delta = layer_1_error * nonlin(layer_1, deriv = TRUE)\n  \n      # update weights\n      synapse_0 = synapse_0 + crossprod(layer_0, l1_delta)\n  }\n  \n  list(layer_1 = layer_1, layer_1_error = layer_1_error, synapse_0 = synapse_0)\n}\n\nfit_nn = nn_1(X, y, synapse_0)\n\nmessage(\"Output After Training: \\n\", \n        paste0(capture.output(cbind(fit_nn$layer_1, y)), collapse = '\\n'))\n\nOutput After Training: \n                 y\n[1,] 0.009670417 0\n[2,] 0.007864211 0\n[3,] 0.993590571 1\n[4,] 0.992115835 1\n\n\n\nX = matrix(\n  c(0, 0, 1,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 1),\n  nrow = 4,\n  ncol = 3,\n  byrow = TRUE\n)\n\ny = matrix(as.integer(xor(X[,1], X[,2])), ncol = 1)  # make the relationship explicit\n\nset.seed(1)\n\n# or do randomly in same fashion\nsynapse_0 = matrix(runif(12, -1, 1), 3, 4)\nsynapse_1 = matrix(runif(12, -1, 1), 4, 1)\n\nWarning in matrix(runif(12, -1, 1), 4, 1): data length differs from size of\nmatrix: [12 != 4 x 1]\n\n# synapse_0\n# synapse_1\n\nnn_2 <- function(\n  X,\n  y,\n  synapse_0_start,\n  synapse_1_start,\n  maxiter = 30000,\n  verbose = TRUE\n) {\n    \n  synapse_0 = synapse_0_start\n  synapse_1 = synapse_1_start\n  \n  for (j in 1:maxiter) {\n    layer_1 = plogis(X  %*% synapse_0)              # 4 x 4\n    layer_2 = plogis(layer_1 %*% synapse_1)         # 4 x 1\n    \n    # how much did we miss the target value?\n    layer_2_error = y - layer_2\n    \n    if (verbose && (j %% 10000) == 0) {\n      message(glue::glue(\"Error: {mean(abs(layer_2_error))}\"))\n    }\n  \n    # in what direction is the target value?\n    # were we really sure? if so, don't change too much.\n    layer_2_delta = (y - layer_2) * (layer_2 * (1 - layer_2))\n    \n    # how much did each l1 value contribute to the l2 error (according to the weights)?\n    layer_1_error = layer_2_delta %*% t(synapse_1)\n    \n    # in what direction is the target l1?\n    # were we really sure? if so, don't change too much.  \n    layer_1_delta = tcrossprod(layer_2_delta, synapse_1) * (layer_1 * (1 - layer_1))\n    \n    # update\n    synapse_1 = synapse_1 + crossprod(layer_1, layer_2_delta)\n    synapse_0 = synapse_0 + crossprod(X, layer_1_delta)\n  }\n  \n  list(\n    layer_1_error = layer_1_error,\n    layer_2_error = layer_2_error,\n    synapse_0 = synapse_0,\n    synapse_1 = synapse_1,\n    layer_1 = layer_1,\n    layer_2 = layer_2\n  )\n}\n\nfit_nn = nn_2(\n  X,\n  y,\n  synapse_0_start = synapse_0,\n  synapse_1_start = synapse_1,\n  maxiter = 30000\n)\n\nError: 0.0105538166393651\n\n\nError: 0.00729252475321203\n\n\nError: 0.00589736374094261\n\nglue::glue('Final error: {round(mean(abs(fit_nn$layer_2_error)), 5)}')\n\nFinal error: 0.0059\n\nround(fit_nn$layer_1, 3)\n\n      [,1]  [,2]  [,3]  [,4]\n[1,] 0.259 0.889 0.364 0.445\n[2,] 0.000 0.037 0.978 0.030\n[3,] 0.946 1.000 0.984 0.020\n[4,] 0.016 0.984 1.000 0.001\n\nround(cbind(fit_nn$layer_2, y), 3)\n\n      [,1] [,2]\n[1,] 0.002    0\n[2,] 0.993    1\n[3,] 0.994    1\n[4,] 0.008    0\n\nround(fit_nn$synapse_0, 3)\n\n       [,1]   [,2]   [,3]   [,4]\n[1,]  3.915  7.364  4.705 -3.669\n[2,] -6.970 -5.351  4.337 -3.242\n[3,] -1.050  2.079 -0.559 -0.221\n\nround(fit_nn$synapse_1, 3)\n\n        [,1]\n[1,]  10.988\n[2,] -10.733\n[3,]   5.576\n[4,]  -2.987\n\n\n\n# input dataset\nX = matrix(\n  c(0, 0, 1,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 1),\n  nrow = 4,\n  ncol = 3,\n  byrow = TRUE\n)\n    \n# output dataset            \ny = matrix(c(0, 1, 1, 0), ncol = 1)\n\nalphas = c(0.001, 0.01, 0.1, 1, 10, 100, 1000)\nhidden_size = 32\n\n# compute sigmoid nonlinearity\nsigmoid = plogis # already part of base R, no function needed\n\n# convert output of sigmoid function to its derivative\nsigmoid_output_to_derivative <- function(output) {\n  output * (1 - output)\n}\n\n\nnn_3 <- function(\n  X,\n  y,\n  hidden_size,\n  alpha,\n  maxiter = 30000,\n  show_messages = FALSE\n) {\n    \n  for (val in alpha) {\n    \n    if(show_messages)\n      message(glue::glue(\"Training With Alpha: {val}\"))\n    \n    set.seed(1)\n    \n    # randomly initialize our weights with mean 0\n    synapse_0 = matrix(runif(3 * hidden_size, -1, 1), 3, hidden_size)\n    synapse_1 = matrix(runif(hidden_size), hidden_size, 1)\n  \n    for (j in 1:maxiter) {\n  \n        # Feed forward through layers input, 1, and 2\n        layer_1 = sigmoid(X %*% synapse_0)\n        layer_2 = sigmoid(layer_1 %*% synapse_1)\n  \n        # how much did we miss the target value?\n        layer_2_error = layer_2 - y\n        \n        if ((j %% 10000) == 0 & show_messages) {\n          message(glue::glue(\"Error after {j} iterations: {mean(abs(layer_2_error))}\"))\n        }\n  \n        # in what direction is the target value?\n        # were we really sure? if so, don't change too much.\n        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n  \n        # how much did each l1 value contribute to the l2 error (according to the weights)?\n        layer_1_error = layer_2_delta %*% t(synapse_1)\n  \n        # in what direction is the target l1?\n        # were we really sure? if so, don't change too much.\n        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n  \n        synapse_1 = synapse_1 - val * crossprod(layer_1, layer_2_delta)\n        synapse_0 = synapse_0 - val * crossprod(X, layer_1_delta)\n    }\n  }\n  \n  list(\n    layer_1_error = layer_1_error,\n    layer_2_error = layer_2_error,\n    synapse_0 = synapse_0,\n    synapse_1 = synapse_1,\n    layer_1 = layer_1,\n    layer_2 = layer_2\n  )\n}\n\nset.seed(1)\n\nfit_nn = nn_3(\n  X,\n  y,\n  hidden_size = 32,\n  maxiter = 30000,\n  alpha   = alphas,\n  show_messages = FALSE\n)\n\nset.seed(1)\n\nfit_nn = nn_3(\n  X,\n  y,\n  hidden_size = 32,\n  alpha = 10,\n  show_messages = TRUE\n)\n\nTraining With Alpha: 10\n\n\nError after 10000 iterations: 0.00483502508464005\n\n\nError after 20000 iterations: 0.00207985328313795\n\n\nError after 30000 iterations: 0.00152092933542719\n\ncbind(round(fit_nn$layer_2, 4), y)\n\n       [,1] [,2]\n[1,] 0.0013    0\n[2,] 0.9985    1\n[3,] 0.9986    1\n[4,] 0.0018    0\n\ncbind(round(fit_nn$layer_2, 4), y)\n\n       [,1] [,2]\n[1,] 0.0013    0\n[2,] 0.9985    1\n[3,] 0.9986    1\n[4,] 0.0018    0"
  }
]