{
  "hash": "69bd9b78cbe2f85cc9162853ad6ab2e6",
  "result": {
    "markdown": "---\ntitle: \"Lasso\"\nauthor: \"Simon-Pierre Boucher\"\ndate: \"2023-02-11\"\ncategories: [R, Code, Lasso]\nimage: \"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso <- function(\n  X,                   # model matrix\n  y,                   # target\n  lambda  = .1,        # penalty parameter\n  soft    = TRUE,      # soft vs. hard thresholding\n  tol     = 1e-6,      # tolerance\n  iter    = 100,       # number of max iterations\n  verbose = TRUE       # print out iteration number\n) {\n  \n  # soft thresholding function\n  soft_thresh <- function(a, b) {\n    out = rep(0, length(a))\n    out[a >  b] = a[a > b] - b\n    out[a < -b] = a[a < -b] + b\n    out\n  }\n  \n  w = solve(crossprod(X) + diag(lambda, ncol(X))) %*% crossprod(X,y)\n  tol_curr = 1\n  J = ncol(X)\n  a = rep(0, J)\n  c_ = rep(0, J)\n  i = 1\n  \n  while (tol < tol_curr && i < iter) {\n    w_old = w \n    a = colSums(X^2)\n    l = length(y)*lambda  # for consistency with glmnet approach\n    c_ = sapply(1:J, function(j)  sum( X[,j] * (y - X[,-j] %*% w_old[-j]) ))\n    if (soft) {\n      for (j in 1:J) {\n        w[j] = soft_thresh(c_[j]/a[j], l/a[j])\n      }\n    }\n    else {\n      w = w_old\n      w[c_< l & c_ > -l] = 0\n    }\n    \n    tol_curr = crossprod(w - w_old)  \n    i = i + 1\n    if (verbose && i%%10 == 0) message(i)\n  }\n  \n  w\n}\n\n#' # Data setup\n#' \n#' \nset.seed(8675309)\nN = 500\np = 10\nX = scale(matrix(rnorm(N*p), ncol=p))\nb = c(.5, -.5, .25, -.25, .125, -.125, rep(0, p-6))\ny = scale(X %*% b + rnorm(N, sd=.5))\nlambda = .1\n\n\n# debugonce(lasso)\n\n#' Note, if `lambda=0`, result is the same as  `lm.fit`.\n#' \n#' \nresult_soft = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol = 1e-12,\n  soft = TRUE\n)\n\nresult_hard = lasso(\n  X,\n  y,\n  lambda = lambda,\n  tol    = 1e-12,\n  soft   = FALSE\n)\n\n\n\n\n#' `glmnet` is by default a mixture of ridge and lasso penalties, setting alpha\n#' = 1 reduces to lasso (alpha=0 would be ridge). We set the lambda to a couple\n#' values while only wanting the one set to the same lambda value as above (s).\n\n\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLe chargement a nécessité le package : Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-6\n```\n:::\n\n```{.r .cell-code}\nglmnet_res = coef(\n  glmnet(\n    X,\n    y,\n    alpha  = 1,\n    lambda = c(10, 1, lambda),\n    thresh = 1e-12,\n    intercept = FALSE\n  ),\n  s = lambda\n)\n\nlibrary(lassoshooting)\n\nls_res = lassoshooting(\n  X = X,\n  y = y,\n  lambda = length(y) * lambda,\n  thr = 1e-12\n)\n\n\n#' # Comparison\n\ndata.frame(\n  lm = coef(lm(y ~ . - 1, data.frame(X))),\n  lasso_soft = result_soft,\n  lasso_hard = result_hard,\n  lspack = ls_res$coef,\n  glmnet = glmnet_res[-1, 1],\n  truth  = b\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              lm  lasso_soft lasso_hard      lspack      glmnet  truth\nX1   0.534988063  0.43542527  0.5348784  0.43542528  0.43552489  0.500\nX2  -0.529993422 -0.42876539 -0.5298847 -0.42876538 -0.42886718 -0.500\nX3   0.234376590  0.12436834  0.2343207  0.12436835  0.12447920  0.250\nX4  -0.294350608 -0.20743074 -0.2942946 -0.20743075 -0.20751883 -0.250\nX5   0.126037566  0.02036410  0.1260132  0.02036407  0.02047015  0.125\nX6  -0.159386728 -0.05501971 -0.1593572 -0.05501969 -0.05512364 -0.125\nX7  -0.016718534  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX8   0.009894575  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX9  -0.005441959  0.00000000  0.0000000  0.00000000  0.00000000  0.000\nX10  0.010561128  0.00000000  0.0000000  0.00000000  0.00000000  0.000\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}