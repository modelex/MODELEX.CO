{"title":"Gradient Descent","markdown":{"yaml":{"title":"Gradient Descent","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Gradient Descent"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"headingText":"' # Gradient Descent Algorithm","containsRefs":false,"markdown":"\n\n```{r}\nset.seed(8675309)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)  # model matrix\n\n\n\n\n\ngd = function(\n  par,\n  X,\n  y,\n  tolerance = 1e-3,\n  maxit     = 1000,\n  stepsize  = 1e-3,\n  adapt     = FALSE,\n  verbose   = TRUE,\n  plotLoss  = TRUE\n  ) {\n  \n  # initialize\n  beta = par; names(beta) = colnames(X)\n  loss = crossprod(X %*% beta - y)\n  tol  = 1\n  iter = 1\n  \n  while(tol > tolerance && iter < maxit){\n    \n    LP   = X %*% beta\n    grad = t(X) %*% (LP - y)\n    betaCurrent = beta - stepsize * grad\n    tol  = max(abs(betaCurrent - beta))\n    beta = betaCurrent\n    loss = append(loss, crossprod(LP - y))\n    iter = iter + 1\n    \n    if (adapt)\n      stepsize = ifelse(\n        loss[iter] < loss[iter - 1],  \n        stepsize * 1.2, \n        stepsize * .8\n      )\n    \n    if (verbose && iter %% 10 == 0)\n      message(paste('Iteration:', iter))\n  }\n  \n  if (plotLoss)\n    plot(loss, type = 'l', bty = 'n')\n  \n  list(\n    par    = beta,\n    loss   = loss,\n    RSE    = sqrt(crossprod(LP - y) / (nrow(X) - ncol(X))), \n    iter   = iter,\n    fitted = LP\n  )\n}\n\n\n#' ## Run\n#' \n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you'd have to fiddle with the `stepsize`, which could \n#' be assessed via cross-validation, or alternatively one can use an\n#' adaptive approach, a simple one of which is implemented in this function.\n\ngd_result = gd(\n  init,\n  X = X,\n  y = y,\n  tolerance = 1e-8,\n  stepsize  = 1e-4,\n  adapt     = TRUE\n)\n\nstr(gd_result)\n\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n\nrbind(\n  gd = round(gd_result$par[, 1], 5),\n  lm = coef(lm(y ~ x1 + x2))\n)\n\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Gradient Descent","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Gradient Descent"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"extensions":{"book":{"multiFile":true}}}}}