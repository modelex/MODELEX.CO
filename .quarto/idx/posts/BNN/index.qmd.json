{"title":"Basic Neural Network","markdown":{"yaml":{"title":"Basic Neural Network","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Basic Neural Network","Neural Network"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"headingText":"output dataset","containsRefs":false,"markdown":"\n\n```{r}\nX = matrix( \n  c(0, 0, 1, \n    0, 1, 1, \n    1, 0, 1, \n    1, 1, 1),\n  nrow  = 4,\n  ncol  = 3,\n  byrow = TRUE\n)\n\ny = c(0, 0, 1, 1)\n\n# seed random numbers to make calculation\n# deterministic (just a good practice)\nset.seed(1)\n\n# initialize weights randomly with mean 0\nsynapse_0 = matrix(runif(3, min = -1, max = 1), 3, 1)\n\n# sigmoid function\nnonlin <- function(x, deriv = FALSE) {\n  if (deriv)\n    x * (1 - x)\n  else\n    plogis(x)\n}\n\n\nnn_1 <- function(X, y, synapse_0, maxiter = 10000) {\n  \n  for (iter in 1:maxiter) {\n  \n      # forward propagation\n      layer_0 = X\n      layer_1 = nonlin(layer_0 %*% synapse_0)\n  \n      # how much did we miss?\n      layer_1_error = y - layer_1\n  \n      # multiply how much we missed by the \n      # slope of the sigmoid at the values in layer_1\n      l1_delta = layer_1_error * nonlin(layer_1, deriv = TRUE)\n  \n      # update weights\n      synapse_0 = synapse_0 + crossprod(layer_0, l1_delta)\n  }\n  \n  list(layer_1 = layer_1, layer_1_error = layer_1_error, synapse_0 = synapse_0)\n}\n\nfit_nn = nn_1(X, y, synapse_0)\n\nmessage(\"Output After Training: \\n\", \n        paste0(capture.output(cbind(fit_nn$layer_1, y)), collapse = '\\n'))\n```\n\n```{r}\nX = matrix(\n  c(0, 0, 1,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 1),\n  nrow = 4,\n  ncol = 3,\n  byrow = TRUE\n)\n\ny = matrix(as.integer(xor(X[,1], X[,2])), ncol = 1)  # make the relationship explicit\n\nset.seed(1)\n\n# or do randomly in same fashion\nsynapse_0 = matrix(runif(12, -1, 1), 3, 4)\nsynapse_1 = matrix(runif(12, -1, 1), 4, 1)\n\n# synapse_0\n# synapse_1\n\nnn_2 <- function(\n  X,\n  y,\n  synapse_0_start,\n  synapse_1_start,\n  maxiter = 30000,\n  verbose = TRUE\n) {\n    \n  synapse_0 = synapse_0_start\n  synapse_1 = synapse_1_start\n  \n  for (j in 1:maxiter) {\n    layer_1 = plogis(X  %*% synapse_0)              # 4 x 4\n    layer_2 = plogis(layer_1 %*% synapse_1)         # 4 x 1\n    \n    # how much did we miss the target value?\n    layer_2_error = y - layer_2\n    \n    if (verbose && (j %% 10000) == 0) {\n      message(glue::glue(\"Error: {mean(abs(layer_2_error))}\"))\n    }\n  \n    # in what direction is the target value?\n    # were we really sure? if so, don't change too much.\n    layer_2_delta = (y - layer_2) * (layer_2 * (1 - layer_2))\n    \n    # how much did each l1 value contribute to the l2 error (according to the weights)?\n    layer_1_error = layer_2_delta %*% t(synapse_1)\n    \n    # in what direction is the target l1?\n    # were we really sure? if so, don't change too much.  \n    layer_1_delta = tcrossprod(layer_2_delta, synapse_1) * (layer_1 * (1 - layer_1))\n    \n    # update\n    synapse_1 = synapse_1 + crossprod(layer_1, layer_2_delta)\n    synapse_0 = synapse_0 + crossprod(X, layer_1_delta)\n  }\n  \n  list(\n    layer_1_error = layer_1_error,\n    layer_2_error = layer_2_error,\n    synapse_0 = synapse_0,\n    synapse_1 = synapse_1,\n    layer_1 = layer_1,\n    layer_2 = layer_2\n  )\n}\n\nfit_nn = nn_2(\n  X,\n  y,\n  synapse_0_start = synapse_0,\n  synapse_1_start = synapse_1,\n  maxiter = 30000\n)\nglue::glue('Final error: {round(mean(abs(fit_nn$layer_2_error)), 5)}')\nround(fit_nn$layer_1, 3)\nround(cbind(fit_nn$layer_2, y), 3)\nround(fit_nn$synapse_0, 3)\nround(fit_nn$synapse_1, 3)\n```\n\n```{r}\n# input dataset\nX = matrix(\n  c(0, 0, 1,\n    0, 1, 1,\n    1, 0, 1,\n    1, 1, 1),\n  nrow = 4,\n  ncol = 3,\n  byrow = TRUE\n)\n    \n# output dataset            \ny = matrix(c(0, 1, 1, 0), ncol = 1)\n\nalphas = c(0.001, 0.01, 0.1, 1, 10, 100, 1000)\nhidden_size = 32\n\n# compute sigmoid nonlinearity\nsigmoid = plogis # already part of base R, no function needed\n\n# convert output of sigmoid function to its derivative\nsigmoid_output_to_derivative <- function(output) {\n  output * (1 - output)\n}\n\n\nnn_3 <- function(\n  X,\n  y,\n  hidden_size,\n  alpha,\n  maxiter = 30000,\n  show_messages = FALSE\n) {\n    \n  for (val in alpha) {\n    \n    if(show_messages)\n      message(glue::glue(\"Training With Alpha: {val}\"))\n    \n    set.seed(1)\n    \n    # randomly initialize our weights with mean 0\n    synapse_0 = matrix(runif(3 * hidden_size, -1, 1), 3, hidden_size)\n    synapse_1 = matrix(runif(hidden_size), hidden_size, 1)\n  \n    for (j in 1:maxiter) {\n  \n        # Feed forward through layers input, 1, and 2\n        layer_1 = sigmoid(X %*% synapse_0)\n        layer_2 = sigmoid(layer_1 %*% synapse_1)\n  \n        # how much did we miss the target value?\n        layer_2_error = layer_2 - y\n        \n        if ((j %% 10000) == 0 & show_messages) {\n          message(glue::glue(\"Error after {j} iterations: {mean(abs(layer_2_error))}\"))\n        }\n  \n        # in what direction is the target value?\n        # were we really sure? if so, don't change too much.\n        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)\n  \n        # how much did each l1 value contribute to the l2 error (according to the weights)?\n        layer_1_error = layer_2_delta %*% t(synapse_1)\n  \n        # in what direction is the target l1?\n        # were we really sure? if so, don't change too much.\n        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)\n  \n        synapse_1 = synapse_1 - val * crossprod(layer_1, layer_2_delta)\n        synapse_0 = synapse_0 - val * crossprod(X, layer_1_delta)\n    }\n  }\n  \n  list(\n    layer_1_error = layer_1_error,\n    layer_2_error = layer_2_error,\n    synapse_0 = synapse_0,\n    synapse_1 = synapse_1,\n    layer_1 = layer_1,\n    layer_2 = layer_2\n  )\n}\n\nset.seed(1)\n\nfit_nn = nn_3(\n  X,\n  y,\n  hidden_size = 32,\n  maxiter = 30000,\n  alpha   = alphas,\n  show_messages = FALSE\n)\n\nset.seed(1)\n\nfit_nn = nn_3(\n  X,\n  y,\n  hidden_size = 32,\n  alpha = 10,\n  show_messages = TRUE\n)\n\ncbind(round(fit_nn$layer_2, 4), y)\ncbind(round(fit_nn$layer_2, 4), y)\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Basic Neural Network","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Basic Neural Network","Neural Network"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"extensions":{"book":{"multiFile":true}}}}}