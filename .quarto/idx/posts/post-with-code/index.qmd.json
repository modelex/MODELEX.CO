{"title":"Linear Regression","markdown":{"yaml":{"title":"Linear Regression","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Linear Regression","OLS"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"headingText":"predictors and response","containsRefs":false,"markdown":"\n\nR code to estimate a linear model\n\n```{r}\nset.seed(123)  # ensures replication\n\nN = 100 # sample size\nk = 2   # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)  \ny = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5)  # increasing N will get estimated values closer to these\n\ndfXy = data.frame(X, y)\n\n\n\n#'\n#' # Functions \n#'\n\n#' A maximum likelihood approach.\nlm_ML = function(par, X, y) {\n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta   = par[-1]                             # coefficients\n  sigma2 = par[1]                              # error variance\n  sigma  = sqrt(sigma2)\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link in the glm sense\n  \n  # calculate likelihood\n  L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood\n#   L =  -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu)    # alternate log likelihood form\n\n  -sum(L)                                      # optim by default is minimization, and we want to maximize the likelihood \n                                               # (see also fnscale in optim.control)\n}\n\n# An approach via least squares loss function.\n\nlm_LS = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link\n  \n  # calculate least squares loss function\n  L = crossprod(y - mu)\n}\n\n\n#' #  Obtain Model Estimates\n#'\n#' Setup for use with optim.\n\nX = cbind(1, X)\n\n#' Initial values. Note we'd normally want to handle the sigma differently as\n#' it's bounded by zero, but we'll ignore for demonstration.  Also sigma2 is not\n#' required for the LS approach as it is the objective function.\n\ninit = c(1, rep(0, ncol(X)))\nnames(init) = c('sigma2', 'intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = lm_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptlmLS = optim(\n  par = init[-1],\n  fn  = lm_LS,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\npars_ML = optlmML$par\npars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par)  # calculate sigma2 and add\n\n\n\n#' #  Comparison\n#' \n#' Compare to `lm` which uses QR decomposition.\n\nmodlm = lm(y ~ ., dfXy)\n\n#' Example\n#' \n# QRX = qr(X)\n# Q = qr.Q(QRX)\n# R = qr.R(QRX)\n# Bhat = solve(R) %*% crossprod(Q, y)\n# alternate: qr.coef(QRX, y)\n\nround(\n  rbind(\n    pars_ML,\n    pars_LS,\n    modlm = c(summary(modlm)$sigma^2, coef(modlm))), \n  digits = 3\n)\n\n#' The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in\n#' the traditional least squares approach; diminishes with increasing N as both\n#' tend toward whatever sd^2 you specify when creating the y response above.\n\n\n#'\n#'Compare to glm, which by default assumes gaussian family with identity link\n#'and uses `lm.fit`.\n#'\nmodglm = glm(y ~ ., data = dfXy)\nsummary(modglm)\n\n\n#' Via normal equations.\ncoefs = solve(t(X) %*% X) %*% t(X) %*% y  # coefficients\n\n#' Compare.\n#' \nsqrt(crossprod(y - X %*% coefs) / (N - k - 1))\nsummary(modlm)$sigma\nsqrt(modglm$deviance / modglm$df.residual) \nc(sqrt(pars_ML[1]), sqrt(pars_LS[1]))\n\n# rerun by adding 3-4 zeros to the N\n\n\n```\n\nThis code uses gradient descent to find the **`beta0`** and **`beta1`** coefficients that minimize the mean squared error (MSE). The learning rate **`alpha`** can be adjusted to influence the speed of learning.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Linear Regression","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Linear Regression","OLS"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"extensions":{"book":{"multiFile":true}}}}}