{"title":"Stochastic Gradient Descent","markdown":{"yaml":{"title":"Stochastic Gradient Descent","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Gradient Descent","Stochastic Gradient Descent"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"headingText":"' # Stochastic Gradient Descent Algorithm","containsRefs":false,"markdown":"\n\n```{r}\nset.seed(1234)\n\nn  = 1000\nx1 = rnorm(n)\nx2 = rnorm(n)\ny  = 1 + .5*x1 + .2*x2 + rnorm(n)\nX  = cbind(Intercept = 1, x1, x2)\n\n\n\n\n\nsgd = function(\n  par,                                      # parameter estimates\n  X,                                        # model matrix\n  y,                                        # target variable\n  stepsize = 1,                             # the learning rate\n  stepsizeTau = 0,                          # if > 0, a check on the LR at early iterations\n  average = FALSE\n){\n  \n  # initialize\n  beta = par\n  names(beta) = colnames(X)\n  betamat = matrix(0, nrow(X), ncol = length(beta))      # Collect all estimates\n  fits = NA                                              # fitted values\n  s = 0                                                  # adagrad per parameter learning rate adjustment\n  loss = NA                                              # Collect loss at each point\n  \n  for (i in 1:nrow(X)) {\n    Xi   = X[i, , drop = FALSE]\n    yi   = y[i]\n    LP   = Xi %*% beta                                   # matrix operations not necessary, \n    grad = t(Xi) %*% (LP - yi)                           # but makes consistent with the  standard gd R file\n    s    = s + grad^2\n    beta = beta - stepsize * grad/(stepsizeTau + sqrt(s))     # adagrad approach\n    \n    if (average & i > 1) {\n      beta =  beta - 1/i * (betamat[i - 1, ] - beta)          # a variation\n    } \n    \n    betamat[i,] = beta\n    fits[i]     = LP\n    loss[i]     = (LP - yi)^2\n  }\n  \n  LP = X %*% beta\n  lastloss = crossprod(LP - y)\n  \n  list(\n    par    = beta,                                       # final estimates\n    parvec = betamat,                                    # all estimates\n    loss   = loss,                                       # observation level loss\n    RMSE   = sqrt(sum(lastloss)/nrow(X)),\n    fitted = fits\n  )\n}\n\n\n\n#' # Run\n\n\n#' Set starting values.\n\ninit = rep(0, 3)\n\n#' For any particular data you might have to fiddle with the `stepsize`, perhaps\n#' choosing one based on cross-validation with old data.\n\nsgd_result = sgd(\n  init,\n  X = X,\n  y = y,\n  stepsize = .1,\n  stepsizeTau = .5,\n  average = FALSE\n)\n\nstr(sgd_result)\n\nsgd_result$par\n\n#' ## Comparison\n#' \n#' We can compare to standard linear regression.\n#' \n# summary(lm(y ~ x1 + x2))\ncoef1 = coef(lm(y ~ x1 + x2))\n\nrbind(\n  sgd_result = sgd_result$par[, 1],\n  lm = coef1\n)\n\n#' ## Visualize Estimates\n#' \nlibrary(tidyverse)\n\ngd = data.frame(sgd_result$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>%\n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter',\n               values_to = 'Value') %>%\n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\nggplot(aes(\n  x = Iteration,\n  y = Value,\n  group = Parameter,\n  color = Parameter\n),\ndata = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration == n), size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    size  = 4,\n    data  = filter(gd, Iteration == n)\n  ) +\n  theme_minimal()\n\n\n\n#' # Add alternately data shift\n\n#' This data includes a shift of the previous data.\n\nset.seed(1234)\n\nn2   = 1000\nx1.2 = rnorm(n2)\nx2.2 = rnorm(n2)\ny2 = -1 + .25*x1.2 - .25*x2.2 + rnorm(n2)\nX2 = rbind(X, cbind(1, x1.2, x2.2))\ncoef2 = coef(lm(y2 ~ x1.2 + x2.2))\ny2 = c(y, y2)\n\nn3    = 1000\nx1.3  = rnorm(n3)\nx2.3  = rnorm(n3)\ny3    = 1 - .25*x1.3 + .25*x2.3 + rnorm(n3)\ncoef3 = coef(lm(y3 ~ x1.3 + x2.3))\n\nX3 = rbind(X2, cbind(1, x1.3, x2.3))\ny3 = c(y2, y3)\n\n\n\n#' ## Run\n\n\nsgd_result2 = sgd(\n  init,\n  X = X3,\n  y = y3,\n  stepsize = 1,\n  stepsizeTau = 0,\n  average = FALSE\n)\n\nstr(sgd_result2)\n\n#' Compare with `lm` for each data part.\n#' \nsgd_result2$parvec[c(n, n + n2, n + n2 + n3), ]\nrbind(coef1, coef2, coef3)\n\n#' Visualize estimates.\n#' \ngd = data.frame(sgd_result2$parvec) %>% \n  mutate(Iteration = 1:n())\n\ngd = gd %>% \n  pivot_longer(cols = -Iteration,\n               names_to = 'Parameter', \n               values_to = 'Value') %>% \n  mutate(Parameter = factor(Parameter, labels = colnames(X)))\n\n\nggplot(aes(x = Iteration,\n           y = Value,\n           group = Parameter,\n           color = Parameter\n           ),\n       data = gd) +\n  geom_path() +\n  geom_point(data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n             size = 3) +\n  geom_text(\n    aes(label = round(Value, 2)),\n    hjust = -.5,\n    angle = 45,\n    data = filter(gd, Iteration %in% c(n, n + n2, n + n2 + n3)),\n    size = 4,\n    show.legend = FALSE\n  ) +\n  theme_minimal()\n\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Stochastic Gradient Descent","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Gradient Descent","Stochastic Gradient Descent"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"extensions":{"book":{"multiFile":true}}}}}