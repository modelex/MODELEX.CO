{"title":"Nelder Mead","markdown":{"yaml":{"title":"Nelder Mead","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","Code","Nelder Mead"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"headingText":"' ## Example","containsRefs":false,"markdown":"\n\n```{r}\nnelder_mead = function(\n  f, \n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha    = 1,\n  gamma    = 2,\n  rho      = 0.5,\n  sigma    = 0.5,\n  verbose  = FALSE\n  ) {\n  # init\n  dim = length(x_start)\n  prev_best = f(x_start)\n  no_improv = 0\n  res = list(list(x_start = x_start, prev_best = prev_best))\n  \n  \n  for (i in 1:dim) {\n    x = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res = append(res, list(list(x_start = x, prev_best = score)))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    idx  = sapply(res, `[[`, 2)\n    res  = res[order(idx)]   # ascending order\n    best = res[[1]][[2]]\n    \n    # break after max_iter\n    if (max_iter > 0 & iters >= max_iter) return(res[[1]])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break) return(res[[1]])\n    \n    # centroid\n    x0 = rep(0, dim)\n    for (tup in 1:(length(res)-1)) {\n      for (i in 1:dim) {\n        x0[i] = x0[i] + res[[tup]][[1]][i] / (length(res)-1)\n      }\n    }\n    \n   # reflection\n   xr = x0 + alpha * (x0 - res[[length(res)]][[1]])\n   rscore = f(xr)\n   if (res[[1]][[2]] <= rscore & \n       rscore < res[[length(res)-1]][[2]]) {\n     res[[length(res)]] = list(xr, rscore)\n     next\n   }\n     \n   # expansion\n   if (rscore < res[[1]][[2]]) {\n     # xe = x0 + gamma*(x0 - res[[length(res)]][[1]])   # issue with this\n     xe = x0 + gamma * (xr - x0)   \n     escore = f(xe)\n     if (escore < rscore) {\n       res[[length(res)]] = list(xe, escore)\n       next\n     } else {\n       res[[length(res)]] = list(xr, rscore)\n       next\n     }\n   }\n   \n   # contraction\n   # xc = x0 + rho*(x0 - res[[length(res)]][[1]])  # issue with wiki consistency for rho values (and optim)\n   xc = x0 + rho * (res[[length(res)]][[1]] - x0)\n   cscore = f(xc)\n   if (cscore < res[[length(res)]][[2]]) {\n     res[[length(res)]] = list(xc, cscore)\n     next\n   }\n   \n   # reduction\n   x1   = res[[1]][[1]]\n   nres = list()\n   for (tup in res) {\n     redx  = x1 + sigma * (tup[[1]] - x1)\n     score = f(redx)\n     nres  = append(nres, list(list(redx, score)))\n   }\n   \n   res = nres\n  }\n}\n\n\n\n\n#' The function to minimize.\n#' \nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\n#' Compare to `optimx`.  You may see warnings.\noptimx::optimx(\n  par = c(0, 0, 0),\n  fn = f,\n  method = \"Nelder-Mead\",\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta = 0.5,\n    maxit = 1000,\n    reltol = 1e-12\n  )\n)\n\n\n\n#' ## A Regression Model\n#' \n#' I find a regression model to be more applicable/intuitive for my needs, so\n#' provide an example for that case.\n#' \n#' \n#' ### Data setup\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n\n#' Least squares loss function.\n\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n# lm estimates\nlm.fit(X, y)$coef\n\nnm_result = nelder_mead(\n  f, \n  runif(ncol(X)), \n  max_iter = 2000,\n  no_improve_thr = 1e-12,\n  verbose = FALSE\n)\n\n#' ### Comparison\n#' Compare to `optimx`.\n\nopt_out = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,  # model function\n  method  = 'Nelder-Mead',\n  control = list(\n    alpha = 1,\n    gamma = 2,\n    beta  = 0.5,\n    #rho\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)\n\nrbind(\n  nm_func = unlist(nm_result),\n  nm_optimx = opt_out[1:7]\n)\n\n\n\n#' # Second version\n#' \n#' This is a more natural R approach in my opinion.\n\nnelder_mead2 = function(\n  f,\n  x_start,\n  step = 0.1,\n  no_improve_thr  = 1e-12,\n  no_improv_break = 10,\n  max_iter = 0,\n  alpha = 1,\n  gamma = 2,\n  rho   = 0.5,\n  sigma = 0.5,\n  verbose = FALSE\n) {\n  \n  # init\n  npar = length(x_start)\n  nc = npar + 1\n  prev_best = f(x_start)\n  no_improv = 0\n  res = matrix(c(x_start, prev_best), ncol = nc)\n  colnames(res) = c(paste('par', 1:npar, sep = '_'), 'score')\n  \n  for (i in 1:npar) {\n    x     = x_start\n    x[i]  = x[i] + step\n    score = f(x)\n    res   = rbind(res, c(x, score))\n  }\n  \n  # simplex iter\n  iters = 0\n  \n  while (TRUE) {\n    # order\n    res  = res[order(res[, nc]), ]   # ascending order\n    best = res[1, nc]\n    \n    # break after max_iter\n    if (max_iter & iters >= max_iter) return(res[1, ])\n    iters = iters + 1\n    \n    # break after no_improv_break iterations with no improvement\n    if (verbose) message(paste('...best so far:', best))\n    \n    if (best < (prev_best - no_improve_thr)) {\n      no_improv = 0\n      prev_best = best\n    } else {\n      no_improv = no_improv + 1\n    }\n    \n    if (no_improv >= no_improv_break)\n      return(res[1, ])\n    \n    nr = nrow(res)\n    \n    # centroid: more efficient than previous double loop\n    x0 = colMeans(res[(1:npar), -nc])\n    \n    # reflection\n    xr = x0 + alpha * (x0 - res[nr, -nc])\n    \n    rscore = f(xr)\n    \n    if (res[1, 'score'] <= rscore & rscore < res[npar, 'score']) {\n      res[nr,] = c(xr, rscore)\n      next\n    }\n    \n    # expansion\n    if (rscore < res[1, 'score']) {\n      xe = x0 + gamma * (xr - x0)\n      escore = f(xe)\n      if (escore < rscore) {\n        res[nr, ] = c(xe, escore)\n        next\n      } else {\n        res[nr, ] = c(xr, rscore)\n        next\n      }\n    }\n    \n    # contraction\n    xc = x0 + rho * (res[nr, -nc] - x0)\n    \n    cscore = f(xc)\n    \n    if (cscore < res[nr, 'score']) {\n      res[nr,] = c(xc, cscore)\n      next\n    }\n    \n    # reduction\n    x1 = res[1, -nc]\n    \n    nres = res\n    \n    for (i in 1:nr) {\n      redx  = x1 + sigma * (res[i, -nc] - x1)\n      score = f(redx)\n      nres[i, ] = c(redx, score)\n    }\n    \n    res = nres\n  }\n}\n\n\n#' ## Example function\n\nf = function(x) {\n  sin(x[1]) * cos(x[2]) * (1 / (abs(x[3]) + 1))\n}\n\nnelder_mead2(\n  f, \n  c(0, 0, 0), \n  max_iter = 1000, \n  no_improve_thr = 1e-12\n)\n\noptimx::optimx(\n  par = c(0, 0, 0), \n  fn = f, \n  method   = \"Nelder-Mead\",\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 1000,\n    reltol = 1e-12\n  )\n)\n\n\n\n#' ## A Regression Model\n\nset.seed(8675309)\nN = 500\nnpreds = 5\nX = cbind(1, matrix(rnorm(N * npreds), ncol = npreds))\nbeta = runif(ncol(X), -1, 1)\ny = X %*% beta + rnorm(nrow(X))\n\n# least squares loss\nf = function(b) {\n  crossprod(y - X %*% b)[,1]  # if using optimx need scalar\n}\n\n\nlm_par = lm.fit(X, y)$coef\n\nnm_par = nelder_mead2(\n  f, \n  runif(ncol(X)),\n  max_iter = 2000,\n  no_improve_thr = 1e-12\n)\n\n\n#' ## Comparison\n#' Compare to `optimx`.\n\nopt_par = optimx::optimx(\n  runif(ncol(X)),\n  fn = f,\n  method   = 'Nelder-Mead',\n  control  = list(\n    alpha  = 1,\n    gamma  = 2,\n    beta   = 0.5,\n    maxit  = 2000,\n    reltol = 1e-12\n  )\n)[1:(npreds + 1)]\n\nrbind(\n  lm = lm_par,\n  nm = nm_par,\n  optimx = opt_par,\n  truth  = beta\n)\n\n\n\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Nelder Mead","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","Code","Nelder Mead"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"extensions":{"book":{"multiFile":true}}}}}