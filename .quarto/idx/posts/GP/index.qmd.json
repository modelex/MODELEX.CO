{"title":"Gaussian Process","markdown":{"yaml":{"title":"Gaussian Process","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Gaussian Process"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"headingText":"The covariance function; here it is the squared exponential kernel.","containsRefs":false,"markdown":"\n\n```{r}\nmuFn = function(x){\n  x = sapply(x, function(x) x=0)\n  x\n}\n\n# l is the horizontal scale, sigmaf is the vertical scale.\n# See ?covSEiso in the gpr package for example, which is also based on Rasmussen and\n# Williams Matlab code (gpml Matlab library)\n\nKfn = function(x, l=1, sigmaf=1){\n    sigmaf * exp( -(1/(2*l^2)) * as.matrix(dist(x, upper=T, diag=T)^2) )\n}\n\n\n#####################\n### Preliminaries ###\n#####################\n\nl = 1           # for l, sigmaf, see note at covariance function\nsigmaf = 1      \nkeps = 1e-8     # see note at Kstarstar\nnprior = 5      # number of prior draws\nnpostpred = 3   # number of posterior predictive draws\n\n##################\n### Prior plot ###\n##################\n\n# data setup\nrequire(MASS)\nxg1 = seq(-5, 5, .2)\nyg1 = mvrnorm(nprior,\n              mu = muFn(xg1),\n              Sigma = Kfn(xg1, l = l, sigmaf = sigmaf))\n\n# plot prior\nlibrary(ggplot2)\nlibrary(reshape2)\n\n# reshape data for plotting\ngdat = melt(data.frame(\n  x = xg1,\n  y = t(yg1),\n  sd = apply(yg1, 2, sd)\n), id = c('x', 'sd'))\n# head(gdat) # inspect if desired\n\ng1 = ggplot(aes(x = x, y = value), data = gdat) +\n  geom_line(aes(group = variable), color = '#FF5500', alpha = .5) +\n  labs(title = 'Prior') +\n  theme_minimal()\n\n# g1\n\n#########################################\n### generate noise-less training data ###\n#########################################\nXtrain = c(-4, -3, -2, -1, 1)\nytrain = sin(Xtrain)\nnTrain = length(Xtrain)\n\nXtest = seq(-5, 5, .2)\nnTest = length(Xtest)\n\n#####################################\n### generate posterior predictive ###\n#####################################\n\n# Create K, K*, and K** matrices as defined in the texts\nK = Kfn(Xtrain, l=l, sigmaf=sigmaf)  \nK_ = Kfn(c(Xtrain, Xtest), l=l, sigmaf=sigmaf)                                 # initial matrix\nKstar = K_[1:nTrain, (nTrain+1):ncol(K_)]                                      # dim = N x N*\ntKstar = t(Kstar)                                                              # dim = N* x N\nKstarstar = K_[(nTrain+1):nrow(K_), (nTrain+1):ncol(K_)] + keps*diag(nTest)    # dim = N* x N*; the keps part is for positive definiteness\nKinv = solve(K)\n\n# calculate posterior mean and covariance\npostMu = muFn(Xtest) + t(Kstar) %*% Kinv %*% (ytrain-muFn(Xtrain))\npostCov = Kstarstar - t(Kstar) %*% Kinv %*% Kstar\ns2 = diag(postCov)\n# R = chol(postCov)  \n# L = t(R)      # L is used in alternative formulation below based on gaussSample.m\n\n# generate draws from posterior predictive\ny2 = data.frame(t(mvrnorm(npostpred, mu=postMu, Sigma=postCov)))\n# y2 = data.frame(replicate(npostpred, postMu + L %*% rnorm(postMu))) # alternative\n\n#################################\n### Posterior predictive plot ###\n#################################\n\n# reshape data for plotting\ngdat = melt(data.frame(x=Xtest, y=y2, selower=postMu-2*sqrt(s2), seupper=postMu+2*sqrt(s2)),\n            id=c('x', 'selower', 'seupper'))\n\ng2 = ggplot(aes(x=x, y=value), data=gdat) + \n  geom_ribbon(aes(ymin=selower, ymax=seupper,group=variable), fill='gray90') +\n  geom_line(aes(group=variable), color='#FF5500') +\n  geom_point(aes(x=Xtrain, y=ytrain), data=data.frame(Xtrain, ytrain)) +\n  labs(title='Posterior Predictive') +\n  theme_minimal()\n\n# g2\n\n####################################################\n### Plot prior and posterior predictive together ###\n####################################################\n\nlibrary(gridExtra)\ngrid.arrange(g1, g2, ncol=2)\n```\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.269","editor":"visual","theme":"cosmo","title-block-banner":true,"title":"Gaussian Process","author":"Simon-Pierre Boucher","date":"2023-02-11","categories":["R","code","Gaussian Process"],"image":"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png"},"extensions":{"book":{"multiFile":true}}}}}