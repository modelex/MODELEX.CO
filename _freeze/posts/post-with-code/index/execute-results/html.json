{
  "hash": "9886e07657fbf07572b4a0b881ceaac7",
  "result": {
    "markdown": "---\ntitle: \"Linear Regression\"\nauthor: \"Simon-Pierre Boucher\"\ndate: \"2023-02-11\"\ncategories: [R, code, Linear Regression, OLS]\nimage: \"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png\"\n---\n\n\nR code to estimate a linear model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)  # ensures replication\n\n# predictors and response\nN = 100 # sample size\nk = 2   # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)  \ny = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5)  # increasing N will get estimated values closer to these\n\ndfXy = data.frame(X, y)\n\n\n\n#'\n#' # Functions \n#'\n\n#' A maximum likelihood approach.\nlm_ML = function(par, X, y) {\n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta   = par[-1]                             # coefficients\n  sigma2 = par[1]                              # error variance\n  sigma  = sqrt(sigma2)\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link in the glm sense\n  \n  # calculate likelihood\n  L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood\n#   L =  -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu)    # alternate log likelihood form\n\n  -sum(L)                                      # optim by default is minimization, and we want to maximize the likelihood \n                                               # (see also fnscale in optim.control)\n}\n\n# An approach via least squares loss function.\n\nlm_LS = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n  mu = LP                                      # identity link\n  \n  # calculate least squares loss function\n  L = crossprod(y - mu)\n}\n\n\n#' #  Obtain Model Estimates\n#'\n#' Setup for use with optim.\n\nX = cbind(1, X)\n\n#' Initial values. Note we'd normally want to handle the sigma differently as\n#' it's bounded by zero, but we'll ignore for demonstration.  Also sigma2 is not\n#' required for the LS approach as it is the objective function.\n\ninit = c(1, rep(0, ncol(X)))\nnames(init) = c('sigma2', 'intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = lm_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptlmLS = optim(\n  par = init[-1],\n  fn  = lm_LS,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\npars_ML = optlmML$par\npars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par)  # calculate sigma2 and add\n\n\n\n#' #  Comparison\n#' \n#' Compare to `lm` which uses QR decomposition.\n\nmodlm = lm(y ~ ., dfXy)\n\n#' Example\n#' \n# QRX = qr(X)\n# Q = qr.Q(QRX)\n# R = qr.R(QRX)\n# Bhat = solve(R) %*% crossprod(Q, y)\n# alternate: qr.coef(QRX, y)\n\nround(\n  rbind(\n    pars_ML,\n    pars_LS,\n    modlm = c(summary(modlm)$sigma^2, coef(modlm))), \n  digits = 3\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        sigma2 intercept    b1    b2\npars_ML  0.219    -0.432 0.133 0.112\npars_LS  0.226    -0.432 0.133 0.112\nmodlm    0.226    -0.432 0.133 0.112\n```\n:::\n\n```{.r .cell-code}\n#' The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in\n#' the traditional least squares approach; diminishes with increasing N as both\n#' tend toward whatever sd^2 you specify when creating the y response above.\n\n\n#'\n#'Compare to glm, which by default assumes gaussian family with identity link\n#'and uses `lm.fit`.\n#'\nmodglm = glm(y ~ ., data = dfXy)\nsummary(modglm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ ., data = dfXy)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-0.93651  -0.33037  -0.06222   0.31068   1.03991  \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.43247    0.04807  -8.997 1.97e-14 ***\nX1           0.13341    0.05243   2.544   0.0125 *  \nX2           0.11191    0.04950   2.261   0.0260 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.2262419)\n\n    Null deviance: 24.444  on 99  degrees of freedom\nResidual deviance: 21.945  on 97  degrees of freedom\nAIC: 140.13\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n\n```{.r .cell-code}\n#' Via normal equations.\ncoefs = solve(t(X) %*% X) %*% t(X) %*% y  # coefficients\n\n#' Compare.\n#' \nsqrt(crossprod(y - X %*% coefs) / (N - k - 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]\n[1,] 0.4756489\n```\n:::\n\n```{.r .cell-code}\nsummary(modlm)$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4756489\n```\n:::\n\n```{.r .cell-code}\nsqrt(modglm$deviance / modglm$df.residual) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4756489\n```\n:::\n\n```{.r .cell-code}\nc(sqrt(pars_ML[1]), sqrt(pars_LS[1]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   sigma2    sigma2 \n0.4684616 0.4756490 \n```\n:::\n\n```{.r .cell-code}\n# rerun by adding 3-4 zeros to the N\n```\n:::\n\n\nThis code uses gradient descent to find the **`beta0`** and **`beta1`** coefficients that minimize the mean squared error (MSE). The learning rate **`alpha`** can be adjusted to influence the speed of learning.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}