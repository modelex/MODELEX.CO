{
  "hash": "685ca6a2944eb9f3e05122eb431b76b8",
  "result": {
    "markdown": "---\ntitle: \"Logistic Regression\"\nauthor: \"Simon-Pierre Boucher\"\ndate: \"2023-02-11\"\ncategories: [R, Code, Logistic Regression]\nimage: \"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png\"\n---\n\n\nR code to estimate a logistic regression model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1235)  # ensures replication\n\n\n# predictors and target\n\nN = 10000 # sample size\nk = 2     # number of desired predictors\nX = matrix(rnorm(N * k), ncol = k)\n\n# the linear predictor\nlp = -.5 + .2 * X[, 1] + .1 * X[, 2] # increasing N will get estimated values closer to these\n\ny = rbinom(N, size = 1, prob = plogis(lp))\n\ndfXy = data.frame(X, y)\n\n\n\n#' \n#' # Functions \n#' \n#' A maximum likelihood approach.\n\nlogreg_ML = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                # coefficients\n  N = nrow(X)\n  \n  # linear predictor\n  LP = X %*% beta                           # linear predictor\n  mu = plogis(LP)                           # logit link\n  \n  # calculate likelihood\n  L = dbinom(y, size = 1, prob = mu, log = TRUE)         # log likelihood\n  #   L =  y*log(mu) + (1 - y)*log(1-mu)    # alternate log likelihood form\n  \n  -sum(L)                                   # optim by default is minimization, and we want to maximize the likelihood \n  # (see also fnscale in optim.control)\n}\n\n# An equivalent approach via exponential loss function.\n\nlogreg_exp = function(par, X, y) {\n  # arguments- \n  # par: parameters to be estimated\n  # X: predictor matrix with intercept column\n  # y: response\n  \n  # setup\n  beta = par                                   # coefficients\n  \n  # linear predictor\n  LP = X %*% beta                              # linear predictor\n\n  # calculate exponential loss function (convert y to -1:1 from 0:1)\n  L = sum(exp(-ifelse(y, 1, -1) * .5 * LP))\n}\n\n\n#' # Obtain Model Estimates\n#' Setup for use with `optim`.\n\nX = cbind(1, X)\n\n# initial values\n\ninit = rep(0, ncol(X))\nnames(init) = c('intercept', 'b1', 'b2')\n\noptlmML = optim(\n  par = init,\n  fn  = logreg_ML,\n  X   = X,\n  y   = y,\n  control = list(reltol = 1e-8)\n)\n\noptglmClass = optim(\n  par = init,\n  fn  = logreg_exp,\n  X   = X,\n  y   = y, \n  control = list(reltol = 1e-15)\n)\n\npars_ML  = optlmML$par\npars_exp = optglmClass$par\n\n\n#' # Comparison\n#' \n#' Compare to `glm`.\n\nmodglm = glm(y ~ ., dfXy, family = binomial)\n\nrbind(\n  pars_ML,\n  pars_exp,\n  pars_GLM = coef(modglm)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          intercept        b1         b2\npars_ML  -0.5117658 0.2378927 0.08019841\npars_exp -0.5114284 0.2368478 0.07907056\npars_GLM -0.5117321 0.2378743 0.08032617\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}