{
  "hash": "e74c99867d1a0003210cccd8b9281885",
  "result": {
    "markdown": "---\ntitle: \"Extreme Learning Machine\"\nauthor: \"Simon-Pierre Boucher\"\ndate: \"2023-02-11\"\ncategories: [R, Code, Extreme Learning Machine]\nimage: \"https://www.clipartmax.com/png/middle/124-1248944_the-future-of-ai-artificial-intelligence-logo.png\"\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nelm <- function(X, y, n_hidden=NULL, active_fun=tanh) {\n  # X: an N observations x p features matrix\n  # y: the target\n  # n_hidden: the number of hidden nodes\n  # active_fun: activation function\n  pp1 = ncol(X) + 1\n  w0 = matrix(rnorm(pp1*n_hidden), pp1, n_hidden)       # random weights\n  h = active_fun(cbind(1, scale(X)) %*% w0)             # compute hidden layer\n  B = MASS::ginv(h) %*% y                               # find weights for hidden layer\n  fit = h %*% B                                         # fitted values\n  list(fit= fit, loss=crossprod(fit - y), B=B, w0=w0)\n}\n\n\n\n# one variable, complex function -------------------------------------------\nlibrary(tidyverse); library(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nLe chargement a nécessité le package : nlme\n\n\nAttachement du package : 'nlme'\n\n\nL'objet suivant est masqué depuis 'package:dplyr':\n\n    collapse\n\n\nThis is mgcv 1.8-41. For overview type 'help(\"mgcv-package\")'.\n```\n:::\n\n```{.r .cell-code}\nset.seed(123)\nn = 5000\nx = runif(n)\n# x = rnorm(n)\nmu = sin(2*(4*x-2)) + 2*exp(-(16^2)*((x-.5)^2))\ny = rnorm(n, mu, .3)\n# qplot(x, y)\nd = data.frame(x,y) \n\nX_ = as.matrix(x, ncol=1)\n\ntest = elm(X_, y, n_hidden=100)\nstr(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 4\n $ fit : num [1:5000, 1] -1.0239 0.7311 -0.413 0.0806 -0.4112 ...\n $ loss: num [1, 1] 442\n $ B   : num [1:100, 1] 217 -608 1408 -1433 -4575 ...\n $ w0  : num [1:2, 1:100] 0.35 0.814 -0.517 -2.692 -1.097 ...\n```\n:::\n\n```{.r .cell-code}\n# qplot(x, y) + geom_line(aes(y=test$fit), color='#1e90ff')\ncor(test$fit[,1], y)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8862518\n```\n:::\n\n```{.r .cell-code}\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8482127\n```\n:::\n\n```{.r .cell-code}\nd %>% \n  mutate(fit_elm = test$fit,\n         fit_gam = fitted(gam_comparison)) %>% \n  ggplot() + \n  geom_point(aes(x, y), alpha=.1) +\n  geom_line(aes(x, y=fit_elm), color='#1e90ff') + \n  geom_line(aes(x, y=fit_gam), color='darkred')\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# motorcycle accident data ------------------------------------------------\n\ndata('mcycle', package='MASS')\nx = mcycle[,1]\nX_ = matrix(x, ncol=1)\ny = mcycle[,2]\n\ntest = elm(X_, y, n_hidden=100)\ncor(test$fit[,1], y)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8122349\n```\n:::\n\n```{.r .cell-code}\ngam_comparison = gam(y~s(x))\nsummary(gam_comparison)$r.sq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7832988\n```\n:::\n\n```{.r .cell-code}\nqplot(x, y) +\n  geom_line(aes(y=test$fit), color='#1e90ff') + \n  geom_line(aes(y=fitted(gam_comparison)), color='darkred')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# add covariates ----------------------------------------------------------\n\nd = gamSim(eg=7, n=10000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGu & Wahba 4 term additive model, correlated predictors\n```\n:::\n\n```{.r .cell-code}\nX_ = as.matrix(d[,2:5])\ny = d[,1]\n\nn_nodes = c(10, 25, 100, 250, 500, 1000)\ntest = lapply(n_nodes, function(n) elm(X_, y, n_hidden=n))       # this will take a few seconds\nfinal_n = which.min(sapply(test, function(x) x$loss))\nbest = test[[final_n]]\n# str(best)\nqplot(best$fit[,1], y, alpha=.2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-3.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(best$fit[,1], y)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7241967\n```\n:::\n\n```{.r .cell-code}\ngam_comparison = gam(y~s(x0) + s(x1) + s(x2) + s(x3), data=d)\ngam.check(gam_comparison)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-4.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 15 iterations.\nThe RMS GCV score gradient at convergence was 9.309879e-07 .\nThe Hessian was positive definite.\nModel rank =  37 / 37 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n        k'  edf k-index p-value\ns(x0) 9.00 4.71    1.00    0.48\ns(x1) 9.00 4.89    1.00    0.35\ns(x2) 9.00 8.96    0.99    0.20\ns(x3) 9.00 1.00    1.00    0.47\n```\n:::\n\n```{.r .cell-code}\nsummary(gam_comparison)$r.sq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6952978\n```\n:::\n\n```{.r .cell-code}\ntest_data0 = gamSim(eg=7)  # default n = 400\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGu & Wahba 4 term additive model, correlated predictors\n```\n:::\n\n```{.r .cell-code}\ntest_data =  cbind(1, scale(test_data0[,2:5]))\n\nelm_prediction = tanh(test_data %*% best$w0) %*% best$B          # remember to use your specific activation function here\ngam_prediction = predict(gam_comparison, newdata=test_data0)\ncor(data.frame(elm_prediction, gam_prediction), test_data0$y)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                    [,1]\nelm_prediction 0.6873090\ngam_prediction 0.7185687\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}