"0","set.seed(123)  # ensures replication"
"0",""
"0","# predictors and response"
"0","N = 100 # sample size"
"0","k = 2   # number of desired predictors"
"0","X = matrix(rnorm(N * k), ncol = k)  "
"0","y = -.5 + .2*X[, 1] + .1*X[, 2] + rnorm(N, sd = .5)  # increasing N will get estimated values closer to these"
"0",""
"0","dfXy = data.frame(X, y)"
"0",""
"0",""
"0",""
"0","#'"
"0","#' # Functions "
"0","#'"
"0",""
"0","#' A maximum likelihood approach."
"0","lm_ML = function(par, X, y) {"
"0","  # par: parameters to be estimated"
"0","  # X: predictor matrix with intercept column"
"0","  # y: response"
"0","  "
"0","  # setup"
"0","  beta   = par[-1]                             # coefficients"
"0","  sigma2 = par[1]                              # error variance"
"0","  sigma  = sqrt(sigma2)"
"0","  N = nrow(X)"
"0","  "
"0","  # linear predictor"
"0","  LP = X %*% beta                              # linear predictor"
"0","  mu = LP                                      # identity link in the glm sense"
"0","  "
"0","  # calculate likelihood"
"0","  L = dnorm(y, mean = mu, sd = sigma, log = TRUE) # log likelihood"
"0","#   L =  -.5*N*log(sigma2) - .5*(1/sigma2)*crossprod(y-mu)    # alternate log likelihood form"
"0",""
"0","  -sum(L)                                      # optim by default is minimization, and we want to maximize the likelihood "
"0","                                               # (see also fnscale in optim.control)"
"0","}"
"0",""
"0","# An approach via least squares loss function."
"0",""
"0","lm_LS = function(par, X, y) {"
"0","  # arguments- "
"0","  # par: parameters to be estimated"
"0","  # X: predictor matrix with intercept column"
"0","  # y: response"
"0","  "
"0","  # setup"
"0","  beta = par                                   # coefficients"
"0","  "
"0","  # linear predictor"
"0","  LP = X %*% beta                              # linear predictor"
"0","  mu = LP                                      # identity link"
"0","  "
"0","  # calculate least squares loss function"
"0","  L = crossprod(y - mu)"
"0","}"
"0",""
"0",""
"0","#' #  Obtain Model Estimates"
"0","#'"
"0","#' Setup for use with optim."
"0",""
"0","X = cbind(1, X)"
"0",""
"0","#' Initial values. Note we'd normally want to handle the sigma differently as"
"0","#' it's bounded by zero, but we'll ignore for demonstration.  Also sigma2 is not"
"0","#' required for the LS approach as it is the objective function."
"0",""
"0","init = c(1, rep(0, ncol(X)))"
"0","names(init) = c('sigma2', 'intercept', 'b1', 'b2')"
"0",""
"0","optlmML = optim("
"0","  par = init,"
"0","  fn  = lm_ML,"
"0","  X   = X,"
"0","  y   = y,"
"0","  control = list(reltol = 1e-8)"
"0",")"
"0",""
"0","optlmLS = optim("
"0","  par = init[-1],"
"0","  fn  = lm_LS,"
"0","  X   = X,"
"0","  y   = y,"
"0","  control = list(reltol = 1e-8)"
"0",")"
"0",""
"0","pars_ML = optlmML$par"
"0","pars_LS = c(sigma2 = optlmLS$value / (N - k - 1), optlmLS$par)  # calculate sigma2 and add"
"0",""
"0",""
"0",""
"0","#' #  Comparison"
"0","#' "
"0","#' Compare to `lm` which uses QR decomposition."
"0",""
"0","modlm = lm(y ~ ., dfXy)"
"0",""
"0","#' Example"
"0","#' "
"0","# QRX = qr(X)"
"0","# Q = qr.Q(QRX)"
"0","# R = qr.R(QRX)"
"0","# Bhat = solve(R) %*% crossprod(Q, y)"
"0","# alternate: qr.coef(QRX, y)"
"0",""
"0","round("
"0","  rbind("
"0","    pars_ML,"
"0","    pars_LS,"
"0","    modlm = c(summary(modlm)$sigma^2, coef(modlm))), "
"0","  digits = 3"
"0",")"
"1","       "
"1"," sigma2"
"1"," intercept"
"1","    b1"
"1","    b2"
"1","
pars_ML"
"1","  0.219"
"1","    -0.432"
"1"," 0.133"
"1"," 0.112"
"1","
pars_LS"
"1","  0.226"
"1","    -0.432"
"1"," 0.133"
"1"," 0.112"
"1","
modlm  "
"1","  0.226"
"1","    -0.432"
"1"," 0.133"
"1"," 0.112"
"1","
"
"0","#' The slight difference in sigma is roughly maxlike dividing by N vs. N-k-1 in"
"0","#' the traditional least squares approach; diminishes with increasing N as both"
"0","#' tend toward whatever sd^2 you specify when creating the y response above."
"0",""
"0",""
"0","#'"
"0","#'Compare to glm, which by default assumes gaussian family with identity link"
"0","#'and uses `lm.fit`."
"0","#'"
"0","modglm = glm(y ~ ., data = dfXy)"
"0","summary(modglm)"
"1","
Call:
"
"1",""
"1","glm(formula = y ~ ., data = dfXy)"
"1",""
"1","

"
"1","Deviance Residuals: 
"
"1","     Min  "
"1","      1Q  "
"1","  Median  "
"1","      3Q  "
"1","     Max  "
"1","
"
"1","-0.93651  "
"1","-0.33037  "
"1","-0.06222  "
"1"," 0.31068  "
"1"," 1.03991  "
"1","
"
"1","
Coefficients:
"
"1","           "
"1"," Estimate"
"1"," Std. Error"
"1"," t value"
"1"," Pr(>|t|)"
"1","    "
"1","
(Intercept)"
"1"," -0.43247"
"1","    0.04807"
"1","  -8.997"
"1"," 1.97e-14"
"1"," ***"
"1","
X1         "
"1","  0.13341"
"1","    0.05243"
"1","   2.544"
"1","   0.0125"
"1"," *  "
"1","
X2         "
"1","  0.11191"
"1","    0.04950"
"1","   2.261"
"1","   0.0260"
"1"," *  "
"1","
"
"1","---
Signif. codes:  "
"1",""
"1","0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1"
"1","
"
"1","
(Dispersion parameter for "
"1",""
"1","gaussian"
"1",""
"1"," family taken to be "
"1",""
"1","0.2262419"
"1",""
"1",")

"
"1",""
"1","    Null deviance: 24.444  on 99  degrees of freedom
"
"1",""
"1","Residual deviance: 21.945  on 97  degrees of freedom
"
"1","AIC: "
"1",""
"1","140.13"
"1",""
"1","

"
"1",""
"1","Number of Fisher Scoring iterations: "
"1",""
"1","2"
"1",""
"1","
"
"1","
"
"0","#' Via normal equations."
"0","coefs = solve(t(X) %*% X) %*% t(X) %*% y  # coefficients"
"0",""
"0","#' Compare."
"0","#' "
"0","sqrt(crossprod(y - X %*% coefs) / (N - k - 1))"
"1","    "
"1","      [,1]"
"1","
[1,]"
"1"," 0.4756489"
"1","
"
"0","summary(modlm)$sigma"
"1","[1]"
"1"," 0.4756489"
"1","
"
"0","sqrt(modglm$deviance / modglm$df.residual) "
"1","[1]"
"1"," 0.4756489"
"1","
"
"0","c(sqrt(pars_ML[1]), sqrt(pars_LS[1]))"
"1","   sigma2 "
"1","   sigma2 "
"1","
"
"1","0.4684616 "
"1","0.4756490 "
"1","
"
"0","# rerun by adding 3-4 zeros to the N"
"0",""
"0",""
