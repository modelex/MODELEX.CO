"0","# input dataset"
"0","X = matrix("
"0","  c(0, 0, 1,"
"0","    0, 1, 1,"
"0","    1, 0, 1,"
"0","    1, 1, 1),"
"0","  nrow = 4,"
"0","  ncol = 3,"
"0","  byrow = TRUE"
"0",")"
"0","    "
"0","# output dataset            "
"0","y = matrix(c(0, 1, 1, 0), ncol = 1)"
"0",""
"0","alphas = c(0.001, 0.01, 0.1, 1, 10, 100, 1000)"
"0","hidden_size = 32"
"0",""
"0","# compute sigmoid nonlinearity"
"0","sigmoid = plogis # already part of base R, no function needed"
"0",""
"0","# convert output of sigmoid function to its derivative"
"0","sigmoid_output_to_derivative <- function(output) {"
"0","  output * (1 - output)"
"0","}"
"0",""
"0",""
"0","nn_3 <- function("
"0","  X,"
"0","  y,"
"0","  hidden_size,"
"0","  alpha,"
"0","  maxiter = 30000,"
"0","  show_messages = FALSE"
"0",") {"
"0","    "
"0","  for (val in alpha) {"
"0","    "
"0","    if(show_messages)"
"0","      message(glue::glue(""Training With Alpha: {val}""))"
"0","    "
"0","    set.seed(1)"
"0","    "
"0","    # randomly initialize our weights with mean 0"
"0","    synapse_0 = matrix(runif(3 * hidden_size, -1, 1), 3, hidden_size)"
"0","    synapse_1 = matrix(runif(hidden_size), hidden_size, 1)"
"0","  "
"0","    for (j in 1:maxiter) {"
"0","  "
"0","        # Feed forward through layers input, 1, and 2"
"0","        layer_1 = sigmoid(X %*% synapse_0)"
"0","        layer_2 = sigmoid(layer_1 %*% synapse_1)"
"0","  "
"0","        # how much did we miss the target value?"
"0","        layer_2_error = layer_2 - y"
"0","        "
"0","        if ((j %% 10000) == 0 & show_messages) {"
"0","          message(glue::glue(""Error after {j} iterations: {mean(abs(layer_2_error))}""))"
"0","        }"
"0","  "
"0","        # in what direction is the target value?"
"0","        # were we really sure? if so, don't change too much."
"0","        layer_2_delta = layer_2_error * sigmoid_output_to_derivative(layer_2)"
"0","  "
"0","        # how much did each l1 value contribute to the l2 error (according to the weights)?"
"0","        layer_1_error = layer_2_delta %*% t(synapse_1)"
"0","  "
"0","        # in what direction is the target l1?"
"0","        # were we really sure? if so, don't change too much."
"0","        layer_1_delta = layer_1_error * sigmoid_output_to_derivative(layer_1)"
"0","  "
"0","        synapse_1 = synapse_1 - val * crossprod(layer_1, layer_2_delta)"
"0","        synapse_0 = synapse_0 - val * crossprod(X, layer_1_delta)"
"0","    }"
"0","  }"
"0","  "
"0","  list("
"0","    layer_1_error = layer_1_error,"
"0","    layer_2_error = layer_2_error,"
"0","    synapse_0 = synapse_0,"
"0","    synapse_1 = synapse_1,"
"0","    layer_1 = layer_1,"
"0","    layer_2 = layer_2"
"0","  )"
"0","}"
"0",""
"0","set.seed(1)"
"0",""
"0","fit_nn = nn_3("
"0","  X,"
"0","  y,"
"0","  hidden_size = 32,"
"0","  maxiter = 30000,"
"0","  alpha   = alphas,"
"0","  show_messages = FALSE"
"0",")"
"0",""
"0","set.seed(1)"
"0",""
"0","fit_nn = nn_3("
"0","  X,"
"0","  y,"
"0","  hidden_size = 32,"
"0","  alpha = 10,"
"0","  show_messages = TRUE"
"0",")"
"2","Training With Alpha: 10
"
"2","Error after 10000 iterations: 0.00483502508464005
"
"2","Error after 20000 iterations: 0.00207985328313795
"
"2","Error after 30000 iterations: 0.00152092933542719
"
"0","cbind(round(fit_nn$layer_2, 4), y)"
"1","    "
"1","   [,1]"
"1"," [,2]"
"1","
[1,]"
"1"," 0.0013"
"1","    0"
"1","
[2,]"
"1"," 0.9985"
"1","    1"
"1","
[3,]"
"1"," 0.9986"
"1","    1"
"1","
[4,]"
"1"," 0.0018"
"1","    0"
"1","
"
"0","cbind(round(fit_nn$layer_2, 4), y)"
"1","    "
"1","   [,1]"
"1"," [,2]"
"1","
[1,]"
"1"," 0.0013"
"1","    0"
"1","
[2,]"
"1"," 0.9985"
"1","    1"
"1","
[3,]"
"1"," 0.9986"
"1","    1"
"1","
[4,]"
"1"," 0.0018"
"1","    0"
"1","
"
