"0","set.seed(1234)"
"0",""
"0","n  = 1000"
"0","x1 = rnorm(n)"
"0","x2 = rnorm(n)"
"0","y  = 1 + .5*x1 + .2*x2 + rnorm(n)"
"0","X  = cbind(Intercept = 1, x1, x2)"
"0",""
"0",""
"0",""
"0","#' # Stochastic Gradient Descent Algorithm"
"0",""
"0",""
"0","sgd = function("
"0","  par,                                      # parameter estimates"
"0","  X,                                        # model matrix"
"0","  y,                                        # target variable"
"0","  stepsize = 1,                             # the learning rate"
"0","  stepsizeTau = 0,                          # if > 0, a check on the LR at early iterations"
"0","  average = FALSE"
"0","){"
"0","  "
"0","  # initialize"
"0","  beta = par"
"0","  names(beta) = colnames(X)"
"0","  betamat = matrix(0, nrow(X), ncol = length(beta))      # Collect all estimates"
"0","  fits = NA                                              # fitted values"
"0","  s = 0                                                  # adagrad per parameter learning rate adjustment"
"0","  loss = NA                                              # Collect loss at each point"
"0","  "
"0","  for (i in 1:nrow(X)) {"
"0","    Xi   = X[i, , drop = FALSE]"
"0","    yi   = y[i]"
"0","    LP   = Xi %*% beta                                   # matrix operations not necessary, "
"0","    grad = t(Xi) %*% (LP - yi)                           # but makes consistent with the  standard gd R file"
"0","    s    = s + grad^2"
"0","    beta = beta - stepsize * grad/(stepsizeTau + sqrt(s))     # adagrad approach"
"0","    "
"0","    if (average & i > 1) {"
"0","      beta =  beta - 1/i * (betamat[i - 1, ] - beta)          # a variation"
"0","    } "
"0","    "
"0","    betamat[i,] = beta"
"0","    fits[i]     = LP"
"0","    loss[i]     = (LP - yi)^2"
"0","  }"
"0","  "
"0","  LP = X %*% beta"
"0","  lastloss = crossprod(LP - y)"
"0","  "
"0","  list("
"0","    par    = beta,                                       # final estimates"
"0","    parvec = betamat,                                    # all estimates"
"0","    loss   = loss,                                       # observation level loss"
"0","    RMSE   = sqrt(sum(lastloss)/nrow(X)),"
"0","    fitted = fits"
"0","  )"
"0","}"
"0",""
"0",""
"0",""
"0","#' # Run"
"0",""
"0",""
"0","#' Set starting values."
"0",""
"0","init = rep(0, 3)"
"0",""
"0","#' For any particular data you might have to fiddle with the `stepsize`, perhaps"
"0","#' choosing one based on cross-validation with old data."
"0",""
"0","sgd_result = sgd("
"0","  init,"
"0","  X = X,"
"0","  y = y,"
"0","  stepsize = .1,"
"0","  stepsizeTau = .5,"
"0","  average = FALSE"
"0",")"
"0",""
"0","str(sgd_result)"
"1","List"
"1",""
"1"," of "
"1",""
"1","5"
"1",""
"1","
"
"1"," "
"1",""
"1","$ "
"1",""
"1","par   "
"1",""
"1",":"
"1"," num [1:3, 1] "
"1",""
"1","1.024 0.537 0.148"
"1",""
"1","
"
"1","  .."
"1",""
"1","- attr(*, ""dimnames"")="
"1","List"
"1",""
"1"," of "
"1",""
"1","2"
"1",""
"1","
"
"1","  .. .."
"1",""
"1","$ "
"1",""
"1",""
"1",""
"1",":"
"1"," chr [1:3] "
"1",""
"1","""Intercept"" ""x1"" ""x2"""
"1",""
"1","
"
"1","  .. .."
"1",""
"1","$ "
"1",""
"1",""
"1",""
"1",":"
"1"," NULL
"
"1"," "
"1",""
"1","$ "
"1",""
"1","parvec"
"1",""
"1",":"
"1"," num [1:1000, 1:3] "
"1",""
"1","-0.06208 -0.00264 0.04781 0.09866 0.08242"
"1",""
"1"," ..."
"1",""
"1","
"
"1"," "
"1",""
"1","$ "
"1",""
"1","loss  "
"1",""
"1",":"
"1"," num [1:1000] "
"1",""
"1","0.67 1.261 1.365 2.043 0.215"
"1",""
"1"," ..."
"1",""
"1","
"
"1"," "
"1",""
"1","$ "
"1",""
"1","RMSE  "
"1",""
"1",":"
"1"," num "
"1",""
"1","1.01"
"1",""
"1","
"
"1"," "
"1",""
"1","$ "
"1",""
"1","fitted"
"1",""
"1",":"
"1"," num [1:1000] "
"1",""
"1","0 -0.0236 -0.0446 -0.2828 0.1634"
"1",""
"1"," ..."
"1",""
"1","
"
"0","sgd_result$par"
"1","         "
"1","      [,1]"
"1","
Intercept"
"1"," 1.0241049"
"1","
x1       "
"1"," 0.5368198"
"1","
x2       "
"1"," 0.1478470"
"1","
"
"0","#' ## Comparison"
"0","#' "
"0","#' We can compare to standard linear regression."
"0","#' "
"0","# summary(lm(y ~ x1 + x2))"
"0","coef1 = coef(lm(y ~ x1 + x2))"
"0",""
"0","rbind("
"0","  sgd_result = sgd_result$par[, 1],"
"0","  lm = coef1"
"0",")"
"1","          "
"1"," Intercept"
"1","        x1"
"1","        x2"
"1","
sgd_result"
"1","  1.024105"
"1"," 0.5368198"
"1"," 0.1478470"
"1","
lm        "
"1","  1.029957"
"1"," 0.5177020"
"1"," 0.1631026"
"1","
"
"0","#' ## Visualize Estimates"
"0","#' "
"0","library(tidyverse)"
"0",""
"0","gd = data.frame(sgd_result$parvec) %>% "
"0","  mutate(Iteration = 1:n())"
"0",""
"0","gd = gd %>%"
"0","  pivot_longer(cols = -Iteration,"
"0","               names_to = 'Parameter',"
"0","               values_to = 'Value') %>%"
"0","  mutate(Parameter = factor(Parameter, labels = colnames(X)))"
"0",""
"0","ggplot(aes("
"0","  x = Iteration,"
"0","  y = Value,"
"0","  group = Parameter,"
"0","  color = Parameter"
"0","),"
"0","data = gd) +"
"0","  geom_path() +"
"0","  geom_point(data = filter(gd, Iteration == n), size = 3) +"
"0","  geom_text("
"0","    aes(label = round(Value, 2)),"
"0","    hjust = -.5,"
"0","    angle = 45,"
"0","    size  = 4,"
"0","    data  = filter(gd, Iteration == n)"
"0","  ) +"
"0","  theme_minimal()"
